## --------- Flume Details --------- ##
#Ref for flume :
http://archive-primary.cloudera.com/cdh5/cdh/5/flume-ng-1.5.0-cdh5.3.2/FlumeUserGuide.html

# Available examples location
ls -al /opt/examples/

# Start agent
flume-ng agent \
 --name a1 \
 --conf /home/cloudera/flume/conf \
 --conf-file /home/cloudera/flume/conf/example.conf

# output will be
# INFO source.NetcatSource: Created serverSocket:sun.nio.ch.ServerSocketChannelImpl[/127.0.0.1:44444]

# Once above agent is running with above output
telnet localhost 44444

# After telnet start, type any message and it will be captured by agent

#Explore documentaion for Sources, Sinks, Channels

# Script to generate logs [can directly access]
start_logs

# Script for stop generating logs [can directly access]
stop_logs


/*
# Simple example from flum website
*/

1. copy given 'example.conf' fine and create new file
- mkdir flume-ex
- create file from example.conf and copy below text
-------------
# example.conf: A single-node Flume configuration

# Name the components on this agent
a1.sources = r1
a1.sinks = k1
a1.channels = c1

# Describe/configure the source
a1.sources.r1.type = netcat
a1.sources.r1.bind = localhost
a1.sources.r1.port = 44444

# Describe the sink
a1.sinks.k1.type = logger

# Use a channel which buffers events in memory
a1.channels.c1.type = memory
a1.channels.c1.capacity = 1000
a1.channels.c1.transactionCapacity = 100

# Bind the source and sink to the channel
a1.sources.r1.channels = c1
a1.sinks.k1.channel = c1
-------------

2.  execute below flume command to run flume agent
flume-ng agent 
\ --name a1 \
 --conf /home/cloudera/flume-ex \
 --conf-file /home/cloudera/flume-ex/example.conf

3. As sour is netcat, we can start Netcat using 'nc <host> <port>' and add some messages
nc 127.0.0.1 44444



/*
Multiplexing (1 source and multiple sync)  using Avro RPC
*/
1. Create conf file 
cp example.comf fmp.conf

2. copy folowing content 
------------------
# fmp.conf: Multiplexing flow, 1 source and multiple sink

# Name the components on this agent  [Seprate names with space]
fmp.sources = loggersource
fmp.sinks = loggersink hdfssink
fmp.channels = loggerchannel hdfschannel

# Describe/configure the source  [In this case only 1 source]
fmp.sources.loggersource.type = exec
fmp.sources.loggersource.command = tail -F /opt/gen_logs/logs/access.log

# Describe logger sink - 1
fmp.sinks.loggersink.type = logger

# Describe hdfs sink - 2
fmp.sinks.hdfssink.type = hdfs
fmp.sinks.hdfssink.hdfs.path = hdfs://quickstart.cloudera/user/cloudera/flume-sink


# Channel 1 - memory
fmp.channels.loggerchannel.type = memory
fmp.channels.loggerchannel.capacity = 1000
fmp.channels.loggerchannel.transactionCapacity = 100

# Channel 2 - file for hdfs sink
fmp.channels.hdfschannel.type = file
fmp.channels.hdfschannel.capacity = 1000
fmp.channels.hdfschannel.transactionCapacity = 100

# Bind the source and sink to the channel
fmp.sources.loggersource.channels = loggerchannel hdfschannel
fmp.sinks.loggersink.channel = loggerchannel
fmp.sinks.hdfssink.channel = hdfschannel
------------------

3. Start agent
flume-ng agent --name fmp \
 --conf /home/cloudera/flume-ex \
 --conf-file /home/cloudera/flume-ex/fmp.conf

4. Verify data comming on screen + hdfs location
hadoop fs -ls hdfs://quickstart.cloudera/user/cloudera/flume-sink

/*
In above Multiplexing example multiple sinks require multiple channels which require lots of resources.

Kafka can solve problem by distributing data to multiple subscriber.

Source1 -> Kafka Queue -> Many subscriber to the Kafka Queue
*/
1. update fmp.conf by adding new params
# fmp.conf: Multiplexing flow using Kafka
# Source1 -> Kafka Queue -> Many subscriber to the Kafka Queue

# Name the components on this agent  [Seprate names with space]
fmp.sources = loggersource
fmp.sinks = kafkasink hdfssink
fmp.channels = kafkachannel hdfschannel

# Describe/configure the source  [In this case only 1 source]
fmp.sources.loggersource.type = exec
fmp.sources.loggersource.command = tail -F /opt/gen_logs/logs/access.log

# Describe kafka sink - 1
fmp.sinks.kafkasink.type = org.apache.flume.sink.kafka.KafkaSink
fmp.sinks.kafkasink.brokerList = quickstart.cloudera:9092
fmp.sinks.kafkasink.topic = test


# Describe hdfs sink - 2
fmp.sinks.hdfssink.type = hdfs
fmp.sinks.hdfssink.hdfs.path = hdfs://quickstart.cloudera/user/cloudera/flume-sink/multiplexing-using-kafka
fmp.sinks.hdfssink.hdfs.fileType = DataStream
fmp.sinks.hdfssink.hdfs.rollInterval = 120
fmp.sinks.hdfssink.hdfs.rollSize = 10485760
fmp.sinks.hdfssink.hdfs.rollCount = 30
fmp.sinks.hdfssink.hdfs.filePrefix = retail
fmp.sinks.hdfssink.hdfs.fileSuffix = .txt
fmp.sinks.hdfssink.hdfs.inUseSuffix = .tmp
fmp.sinks.hdfssink.hdfs.useLocalTimeStamp = true 


# Channel 1 - memory
fmp.channels.kafkachannel.type = memory
fmp.channels.kafkachannel.capacity = 1000
fmp.channels.kafkachannel.transactionCapacity = 100

# Channel 2 - file for hdfs sink
fmp.channels.hdfschannel.type = file
fmp.channels.hdfschannel.capacity = 1000
fmp.channels.hdfschannel.transactionCapacity = 100

# Bind the source and sink to the channel
fmp.sources.loggersource.channels = kafkachannel hdfschannel
fmp.sinks.kafkasink.channel = kafkachannel
fmp.sinks.hdfssink.channel = hdfschannel


2. Start flume-ng agent
flume-ng agent --name fmp \
 --conf /home/cloudera/flume-ex \
 --conf-file /home/cloudera/flume-ex/fmp.conf

4. Verify data comming on screen + hdfs location
hadoop fs -ls hdfs://quickstart.cloudera/user/cloudera/flume-sink


/*
SparkStreaming using flume as sink

*/
1. copy required 3 plugin jar files to flume[search integraion on spark-programming-guide]
ref: https://spark.apache.org/docs/1.6.2/streaming-flume-integration.html

- find flume locatio by 'whereis flume-ng' and copy all 3 jars using 'wget <url>'

2. Make copy of existing fmp.conf and create new one fkmp.conf and edit  ~/flume-ex/fkmp.conf

# fsmp.conf: Multiplexing flow using spark
# Source1 -> spark Queue -> Many subscriber to the spark Queue

# Name the components on this agent  [Seprate names with space]
fsmp.sources = loggersource
fsmp.sinks = sparksink hdfssink
fsmp.channels = sparkchannel hdfschannel

# Describe/configure the source  [In this case only 1 source]
fsmp.sources.loggersource.type = exec
fsmp.sources.loggersource.command = tail -F /opt/gen_logs/logs/access.log

# Describe spark sink - 1
fsmp.sinks.sparksink.type = org.apache.spark.streaming.flume.sink.SparkSink
fsmp.sinks.sparksink.hostname = quickstart.cloudera
fsmp.sinks.sparksink.port = 19999

# Describe hdfs sink - 2
fsmp.sinks.hdfssink.type = hdfs
fsmp.sinks.hdfssink.hdfs.path = hdfs://quickstart.cloudera/user/cloudera/flume-sink/multiplexing-using-spark
fsmp.sinks.hdfssink.hdfs.fileType = DataStream
fsmp.sinks.hdfssink.hdfs.rollInterval = 120
fsmp.sinks.hdfssink.hdfs.rollSize = 10485760
fsmp.sinks.hdfssink.hdfs.rollCount = 30
fsmp.sinks.hdfssink.hdfs.filePrefix = retail
fsmp.sinks.hdfssink.hdfs.fileSuffix = .txt
fsmp.sinks.hdfssink.hdfs.inUseSuffix = .tmp
fsmp.sinks.hdfssink.hdfs.useLocalTimeStamp = true 


# Channel 1 - memory
fsmp.channels.sparkchannel.type = memory
fsmp.channels.sparkchannel.capacity = 1000
fsmp.channels.sparkchannel.transactionCapacity = 100

# Channel 2 - file for hdfs sink
fsmp.channels.hdfschannel.type = file
fsmp.channels.hdfschannel.capacity = 1000
fsmp.channels.hdfschannel.transactionCapacity = 100

# Bind the source and sink to the channel
fsmp.sources.loggersource.channels = sparkchannel hdfschannel
fsmp.sinks.sparksink.channel = sparkchannel
fsmp.sinks.hdfssink.channel = hdfschannel


3. Start flume-ng with folliwng command

flume-ng agent --name fsmp \
 --conf /home/cloudera/flume \
 --conf-file /home/cloudera/flume/fsmp.conf

4. Start spark-shell and execute following commands
import org.apache.spark.SparkConf
import org.apache.spark.streaming._
import org.apache.spark.streaming.flume._
import org.apache.spark.util.IntParam

/* 
When we start spark-shell we already have 'sc' object so we can not create another SparkContext without stopping existing one.
	To avoide errors we can use existing 'sc' amd set required parameters.
*/ 
sc.getConf.setMaster("local[2]").setAppName("NetworkWordCount").set("spark.ui.port", "44040" )

// Create a Streaming context with 2 second batch interval using existing 'sc'
val ssc = new StreamingContext(sc, Seconds(10) )

/* Create a flume stream that polls the Spark Sink running in a Flume agent
Host and port must be same that we set in flume.conf file
fsmp.sinks.sparksink.hostname = "quickstart.cloudera"
fsmp.sinks.sparksink.port = 19999
*/
val stream = FlumeUtils.createPollingStream(ssc, "quickstart.cloudera", 19999)

// Print out the count of events received from this server in each batch
stream.count().map(cnt => "Received " + cnt + " flume events." ).print()

ssc.start()
ssc.awaitTermination()