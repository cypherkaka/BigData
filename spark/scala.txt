/**
Spark using Scala context
**/

/*
Build jar file using sbt
*/
download sbt and set environment variable
/home/cloudera/sbt/bin

//Append following two line to end of nano ~/.bash_profile
SBT_HOME=/home/cloudera/sbt    
export PATH=$PATH:$SBT_HOME/bin

// Run following command to reload bash_profile
[cloudera@quickstart ~]$ . ~/.bash_profile 

// Submit spark job
spark-submit --class "SimpleApp" \
--master local \
/home/cloudera/scala-ex1/target/scala-2.10/simple-project_2.10-1.0.jar

/**
Read data from HDFS and write it back to HDFS
**/
val dataRDD = sc.textFile("/user/cloudera/sqoop_import/departments")
val dataRDD = sc.textFile("hdfs://quickstart.cloudera:8022/user/cloudera/sqoop_import/departments")
dataRDD.count()

// To print each records
dataRDD.collect.foreach(println)

// Store data back to hdfs
dataRDD.saveAsTextFile("hdfs://quickstart.cloudera:8022/user/cloudera/spark/departments")
dataRDD.saveAsObjectFile("hdfs://quickstart.cloudera:8022/user/cloudera/spark/departmentsObject")


/*
Reading and saving sequence file
*/
// Import below package witch include all key types like NullWritable.class 
import org.apache.hadoop.io._

val dataRDD = sc.textFile("/user/cloudera/sqoop_import/departments")
dataRDD.collect.foreach(println)

dataRDD.map(rec => (NullWritable.get(), rec)).saveAsSequenceFile("/user/cloudera/scalaspark/departmentsSeq")
dataRDD.map(rec => (rec.split(",")(0), rec.split(",")(1))).saveAsSequenceFile("/user/cloudera/scalaspark/departmentsSeq")

// reading sequence file with NullWritable as key
sc.sequenceFile("/user/cloudera/scalaspark/departmentsSeq", classOf[NullWritable], classOf[Text]).map(rec => rec.toString()).collect().foreach(println)

// reading sequence file with Text as key
sc.sequenceFile("/user/cloudera/scalaspark/departmentsSeq", classOf[Text], classOf[Text]).map(rec => rec.toString()).collect().foreach(println)

/*
Reading and saving JSon from SQLContext
*/
import org.apache.spark.sql.hive.HiveContext
val sqlContext = new HiveContext(sc)
val departmentsRDD = sqlContext.sql("select * from departments")
departmentsRDD.collect().foreach(println)

// copy following departments.json to hdfs
/*
{"department_id":1, "department_name":"A"}
{"department_id":2, "department_name":"B"}
{"department_id":3, "department_name":"C"}
{"department_id":4, "department_name":"D"}
*/

// copy to hdfs
hadoop fs -put departments.json /user/cloudera/scalaspark

// register table from Json
import org.apache.spark.sql.SQLContext
val sqlContext = new SQLContext(sc)
val departmentsJson = sqlContext.jsonFile("/user/cloudera/scalaspark/departments.json")
departmentsJson.registerTempTable("departmentsTable")
val departmentsData = sqlContext.sql("select * from departmentsTable")
departmentsData.collect().foreach(println)

// Writing data in json format
departmentsData.toJSON.saveAsTextFile("/user/cloudera/scalaspark/departmentsJson")

/**
Word count using flatMap, map, reduceByKey
*/
val data = sc.textFile("/user/cloudera/wordcount.txt") 
val dataFlatMap = data.flatMap(rec => rec.split(" "))
val dataMap = dataFlatMap.map(rec => (rec,1))  
val dataReduceByKey = dataMap.reduceByKey((acc, value) => acc + value)
dataReduceByKey.saveAsTextFile("/user/cloudera/wordcountoutput")

/**
Joining data sets: only requrement is 'common key' between two transformation
*/

//Check database tables to join [orders, order_items]
[cloudera@quickstart ~]$ mysql -u retail_dba -p
mysql> show databases
mysql> use retail_db
mysql> show tables

// Problem: Get revenue and number of orders from order_items on daily basis
val ordersRDD = sc.textFile("/user/cloudera/sqoop_import/orders")
val orderItemsRDD = sc.textFile("/user/cloudera/sqoop_import/order_items")

val ordersParsedRDD = ordersRDD.map(rec => (rec.split(",")(0).toInt, rec))
val orderItemsParsedRDD = orderItemsRDD.map(rec => (rec.split(",")(1).toInt, rec))

val ordersJoinOrderItems = orderItemsParsedRDD.join(ordersParsedRDD)
/*  Output:
scala> ordersJoinOrderItems.take(5).foreach(println)
(41234,(102921,41234,249,2,109.94,54.97,41234,2014-04-04 00:00:00.0,3182,PENDING_PAYMENT))
(65722,(164249,65722,365,2,119.98,59.99,65722,2014-05-23 00:00:00.0,4077,COMPLETE))
(65722,		(164250,65722,730,5,400.0,80.0,		65722,2014-05-23 00:00:00.0,4077,COMPLETE))
------		-------------------------------------------------------------------------------
  _1											_2

(65722,		(164252,65722,627,5,199.95,39.99,  	65722,2014-05-23 00:00:00.0,4077,COMPLETE))
------		---------------------------------	------------------------------------------
_1         				_1										_2
OrderId	       		ORDER_ITEMS  						    	ORDERS	

tuples can be access using ._ and array can be accessed using (1) notation
*/

// <orderDate, totalRevenue>
val revenuePerOrderPerDay = ordersJoinOrderItems.map(t => (t._2._2.split(",")(1), t._2._1.split(",")(4).toFloat))

// revenue per day from joined data
val totalRevenuePerDay = revenuePerOrderPerDay.reduceByKey((t1,t2) => t1+t2)
totalRevenuePerDay.sortByKey().take(10).foreach(println)
/* Output:
(2013-07-25 00:00:00.0,68153.875)                                               
(2013-07-26 00:00:00.0,136520.1)
(2013-07-27 00:00:00.0,101074.33)
(2013-07-28 00:00:00.0,87123.086)
(2013-07-29 00:00:00.0,137287.02)
(2013-07-30 00:00:00.0,102745.59)
(2013-07-31 00:00:00.0,131878.02)
(2013-08-01 00:00:00.0,129001.586)
(2013-08-02 00:00:00.0,109346.99)
(2013-08-03 00:00:00.0,95266.89)   
*/

// Number or orders from order_items on daily basis
// Order count per day
val ordersPerDay = ordersJoinOrderItems.map(t => (t._2._2.split(",")(1) + "," + t._1)).distinct()
val ordersPerDayParsedRDD = ordersPerDay.map(t => (t.split(",")(0), 1))
val totalOrdersPerDay = ordersPerDayParsedRDD.reduceByKey((x,y) => x+y)
totalOrdersPerDay.sortByKey().take(10).foreach(println)
/* Output:
(2013-07-25 00:00:00.0,116)                                                     
(2013-07-26 00:00:00.0,233)
(2013-07-27 00:00:00.0,175)
(2013-07-28 00:00:00.0,158)
(2013-07-29 00:00:00.0,216)
(2013-07-30 00:00:00.0,182)
(2013-07-31 00:00:00.0,209)
(2013-08-01 00:00:00.0,212)
(2013-08-02 00:00:00.0,186)
(2013-08-03 00:00:00.0,159)
*/
 
// Join order count per day and revenue per day
val finalJoinRDD = totalOrdersPerDay.join(totalRevenuePerDay)

finalJoinRDD.sortByKey().take(10).foreach(println)
/* Outpu:
 (2013-07-25 00:00:00.0,(116,68153.875))                                         
(2013-07-26 00:00:00.0,(233,136520.1))
(2013-07-27 00:00:00.0,(175,101074.33))
(2013-07-28 00:00:00.0,(158,87123.086))
(2013-07-29 00:00:00.0,(216,137287.02))
(2013-07-30 00:00:00.0,(182,102745.59))
(2013-07-31 00:00:00.0,(209,131878.02))
(2013-08-01 00:00:00.0,(212,129001.586))
(2013-08-02 00:00:00.0,(186,109346.99))
(2013-08-03 00:00:00.0,(159,95266.89))
*/


/**
Join data sets using hive and sql
*/

// For HiveContext [used when we have hive tables]
import org.apache.spark.sql.hive.HiveContext
val sqlContext = new HiveContext(sc)

// For SQLContext we also require Row [used to create temp tables]
import org.apache.spark.sql.SQLContext
import org.apache.spark.sql.Row 
val sqlContext = new SQLContext(sc)

//set parameter to limit number of partition, by default it use 200
sqlContext.sql("set spark.sql.shuffel.partitions=10")

sqlContext.sql("select * from orders limit 10").collect().foreach(println);

sqlContext.sql("select o.order_date, round(sum(oi.order_item_subtotal), 2), count(distinct(oi.order_item_order_id))
from orders o join order_items oi on o.order_id = oi.order_item_order_id 
group by o.order_date 
order by o.order_date").take(10).foreach(println)


/**
// Execute query agains strutcuted data using SQLContext
// [Read HDFS and create temp tables and execute query against it]
*/

import org.apache.spark.sql.SQLContext
import org.apache.spark.sql.Row 
val sqlContext = new SQLContext(sc)
sqlContext.sql("set spark.sql.shuffel.partitions=10")

//Create Orders
case class Orders(
order_id: Int,
order_date: String,
order_customer_id: Int,
order_status: String
)

val ordersRDD = sc.textFile("/user/cloudera/sqoop_import/orders").
map(o => o.split(",")).
map(o => Orders(o(0).toInt, o(1), o(2).toInt, o(3))).toDF()

// required to register temp tables
import sqlContext.implicits._
ordersRDD.registerTempTable("orders")

// verify records
sqlContext.sql("select * from orders limit 10").collect().foreach(println)

// Create OrderItems
case class OrderItems(
order_item_id: Int,
order_item_order_id: Int,
order_item_product_id: Int,
order_item_quantity: Int,
order_item_subtotal: Float,
order_item_product_price: Float
)

val orderItemsRDD = sc.textFile("/user/cloudera/sqoop_import/order_items").
map(rec => rec.split(",")).
map(oi => OrderItems(oi(0).toInt, oi(1).toInt, oi(2).toInt, oi(3).toInt, oi(4).toFloat, oi(5).toFloat)).toDF()

orderItemsRDD.registerTempTable("order_items")
  
val joinAggData = sqlContext.sql("select o.order_date, round(sum(oi.order_item_subtotal), 2), " +
"count(distinct o.order_id) from orders o join order_items oi " +
"on o.order_id = oi.order_item_order_id " +
"group by o.order_date order by o.order_date")

sqlContext.sql("set spark.sql.shuffel.partitions=10")
joinAggData.collect().foreach(println)
