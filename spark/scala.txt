/**
Spark using Scala context
**/

/*
Build jar file using sbt
*/
download sbt and set environment variable
/home/cloudera/sbt/bin

//Append following two line to end of nano ~/.bash_profile
SBT_HOME=/home/cloudera/sbt    
export PATH=$PATH:$SBT_HOME/bin

// Run following command to reload bash_profile
[cloudera@quickstart ~]$ . ~/.bash_profile 

// Submit spark job
spark-submit --class "SimpleApp" \
--master local \
/home/cloudera/scala-ex1/target/scala-2.10/simple-project_2.10-1.0.jar

/**
Read data from HDFS and write it back to HDFS
**/
val dataRDD = sc.textFile("/user/cloudera/sqoop_import/departments")
val dataRDD = sc.textFile("hdfs://quickstart.cloudera:8022/user/cloudera/sqoop_import/departments")
dataRDD.count()

// To print each records
dataRDD.collect.foreach(println)

// Store data back to hdfs
dataRDD.saveAsTextFile("hdfs://quickstart.cloudera:8022/user/cloudera/spark/departments")
dataRDD.saveAsObjectFile("hdfs://quickstart.cloudera:8022/user/cloudera/spark/departmentsObject")


/*
Reading and saving sequence file
*/
// Import below package witch include all key types like NullWritable.class 
import org.apache.hadoop.io._

val dataRDD = sc.textFile("/user/cloudera/sqoop_import/departments")
dataRDD.collect.foreach(println)

dataRDD.map(rec => (NullWritable.get(), rec)).saveAsSequenceFile("/user/cloudera/scalaspark/departmentsSeq")
dataRDD.map(rec => (rec.split(",")(0), rec.split(",")(1))).saveAsSequenceFile("/user/cloudera/scalaspark/departmentsSeq")

// reading sequence file with NullWritable as key
sc.sequenceFile("/user/cloudera/scalaspark/departmentsSeq", classOf[NullWritable], classOf[Text]).map(rec => rec.toString()).collect().foreach(println)

// reading sequence file with Text as key
sc.sequenceFile("/user/cloudera/scalaspark/departmentsSeq", classOf[Text], classOf[Text]).map(rec => rec.toString()).collect().foreach(println)

/*
*/
import org.apache.spark.sql.hive.HiveContext
val sqlContext = new HiveContext(sc)
val departmentsRDD = sqlContext.sql("select * from departments")
departmentsRDD.collect().foreach(println)

// copy following departments.json to hdfs
/*
{"department_id":1, "department_name":"A"}
{"department_id":2, "department_name":"B"}
{"department_id":3, "department_name":"C"}
{"department_id":4, "department_name":"D"}
*/

// copy to hdfs
hadoop fs -put departments.json /user/cloudera/scalaspark

// register table from Json
import org.apache.spark.sql.SQLContext
val sqlContext = new SQLContext(sc)
val departmentsJson = sqlContext.jsonFile("/user/cloudera/scalaspark/departments.json")
departmentsJson.registerTempTable("departmentsTable")
val departmentsData = sqlContext.sql("select * from departmentsTable")
departmentsData.collect().foreach(println)

// Writing data in json format
departmentsData.toJSON.saveAsTextFile("/user/cloudera/scalaspark/departmentsJson")






