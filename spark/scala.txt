/**
Spark using Scala context
**/

/****  Basic Scala  **** 

scala> val list = List(3,4,9,0,1,2,0,8,3,4,1,2,6,4,9,0,9)
list: List[Int] = List(3, 4, 9, 0, 1, 2, 0, 8, 3, 4, 1, 2, 6, 4, 9, 0, 9)

scala> val list: List[Int] = List(3,4,9,0,1,2,0,8,3,4,1,2,6,4,9,0,9)
list: List[Int] = List(3, 4, 9, 0, 1, 2, 0, 8, 3, 4, 1, 2, 6, 4, 9, 0, 9)

scala> list.length
res0: Int = 17

scala> list.distinct
res2: List[Int] = List(3, 4, 9, 0, 1, 2, 8, 6)

scala> list.sorted
res3: List[Int] = List(0, 0, 0, 1, 1, 2, 2, 3, 3, 4, 4, 4, 6, 8, 9, 9, 9)

scala> list.reverse
res4: List[Int] = List(9, 0, 9, 4, 6, 2, 1, 4, 3, 8, 0, 2, 1, 0, 9, 4, 3)

scala> list.sorted.reverse
res5: List[Int] = List(9, 9, 9, 8, 6, 4, 4, 4, 3, 3, 2, 2, 1, 1, 0, 0, 0)

scala> list.distinct.sorted.reverse
res6: List[Int] = List(9, 8, 6, 4, 3, 2, 1, 0)

scala> list.distinct.sorted.reverse.take(4)
res7: List[Int] = List(9, 8, 6, 4)

scala> list(1)
res8: Int = 4

scala> //to add all elements

scala> list.reduce(_ + _)
res9: Int = 65

scala> val t:(Int, Int)=(2,8)
t: (Int, Int) = (2,8)

scala> t._1
res10: Int = 2

scala> t._2
res11: Int = 8

scala> val t: (Int, List[Int]) = (1, List(3,8,1,1,7,8))
t: (Int, List[Int]) = (1,List(3, 8, 1, 1, 7, 8))

scala> t._2
res12: List[Int] = List(3, 8, 1, 1, 7, 8)

scala> t._2(4)
res13: Int = 7

scala> val t: (Int, (List[Int], List[String])) = (1, (List(2,9,0,2,6,2,0), List("Hello", "World")))
t: (Int, (List[Int], List[String])) = (1,(List(2, 9, 0, 2, 6, 2, 0),List(Hello, World)))

scala> t._1
res14: Int = 1

scala> t._2
res15: (List[Int], List[String]) = (List(2, 9, 0, 2, 6, 2, 0),List(Hello, World))

scala> t._2._2
res16: List[String] = List(Hello, World)

scala> t._2._2(1)
res17: String = World
*/

/*
Build jar file using sbt
*/
download sbt and set environment variable
/home/cloudera/sbt/bin

//Append following two line to end of nano ~/.bash_profile
SBT_HOME=/home/cloudera/sbt    
export PATH=$PATH:$SBT_HOME/bin

// Run following command to reload bash_profile
[cloudera@quickstart ~]$ . ~/.bash_profile 

// Submit spark job
spark-submit --class "SimpleApp" \
--master local \
/home/cloudera/scala-ex1/target/scala-2.10/simple-project_2.10-1.0.jar

/**
Read data from HDFS and write it back to HDFS
**/
val dataRDD = sc.textFile("/user/cloudera/sqoop_import/departments")
val dataRDD = sc.textFile("hdfs://quickstart.cloudera:8022/user/cloudera/sqoop_import/departments")
dataRDD.count()

// To print each records
dataRDD.collect.foreach(println)

// Store data back to hdfs
dataRDD.saveAsTextFile("hdfs://quickstart.cloudera:8022/user/cloudera/spark/departments")
dataRDD.saveAsObjectFile("hdfs://quickstart.cloudera:8022/user/cloudera/spark/departmentsObject")


/*
Reading and saving sequence file
*/
// Import below package witch include all key types like NullWritable.class 
import org.apache.hadoop.io._

val dataRDD = sc.textFile("/user/cloudera/sqoop_import/departments")
dataRDD.collect.foreach(println)

dataRDD.map(rec => (NullWritable.get(), rec)).saveAsSequenceFile("/user/cloudera/scalaspark/departmentsSeq")
dataRDD.map(rec => (rec.split(",")(0), rec.split(",")(1))).saveAsSequenceFile("/user/cloudera/scalaspark/departmentsSeq")

// reading sequence file with NullWritable as key
sc.sequenceFile("/user/cloudera/scalaspark/departmentsSeq", classOf[NullWritable], classOf[Text]).map(rec => rec.toString()).collect().foreach(println)

// reading sequence file with Text as key
sc.sequenceFile("/user/cloudera/scalaspark/departmentsSeq", classOf[Text], classOf[Text]).map(rec => rec.toString()).collect().foreach(println)

/*
Reading and saving JSon from SQLContext
*/
import org.apache.spark.sql.hive.HiveContext
val sqlContext = new HiveContext(sc)
val departmentsRDD = sqlContext.sql("select * from departments")
departmentsRDD.collect().foreach(println)

// copy following departments.json to hdfs
/*
{"department_id":1, "department_name":"A"}
{"department_id":2, "department_name":"B"}
{"department_id":3, "department_name":"C"}
{"department_id":4, "department_name":"D"}
*/

// copy to hdfs
hadoop fs -put departments.json /user/cloudera/scalaspark

// register table from Json
import org.apache.spark.sql.SQLContext
val sqlContext = new SQLContext(sc)
val departmentsJson = sqlContext.jsonFile("/user/cloudera/scalaspark/departments.json")
departmentsJson.registerTempTable("departmentsTable")
val departmentsData = sqlContext.sql("select * from departmentsTable")
departmentsData.collect().foreach(println)

// Writing data in json format
departmentsData.toJSON.saveAsTextFile("/user/cloudera/scalaspark/departmentsJson")

/**
Word count using flatMap, map, reduceByKey
*/
val data = sc.textFile("/user/cloudera/wordcount.txt") 
val dataFlatMap = data.flatMap(rec => rec.split(" "))
val dataMap = dataFlatMap.map(rec => (rec,1))  
val dataReduceByKey = dataMap.reduceByKey((acc, value) => acc + value)
dataReduceByKey.saveAsTextFile("/user/cloudera/wordcountoutput")

/**
Joining data sets: only requrement is 'common key' between two transformation
*/

//Check database tables to join [orders, order_items]
[cloudera@quickstart ~]$ mysql -u retail_dba -p
mysql> show databases
mysql> use retail_db
mysql> show tables

// Problem: Get revenue and number of orders from order_items on daily basis
val ordersRDD = sc.textFile("/user/cloudera/sqoop_import/orders")
val orderItemsRDD = sc.textFile("/user/cloudera/sqoop_import/order_items")

val ordersParsedRDD = ordersRDD.map(rec => (rec.split(",")(0).toInt, rec))
val orderItemsParsedRDD = orderItemsRDD.map(rec => (rec.split(",")(1).toInt, rec))

val ordersJoinOrderItems = orderItemsParsedRDD.join(ordersParsedRDD)
/*  Output:
scala> ordersJoinOrderItems.take(5).foreach(println)
(41234,(102921,41234,249,2,109.94,54.97,41234,2014-04-04 00:00:00.0,3182,PENDING_PAYMENT))
(65722,(164249,65722,365,2,119.98,59.99,65722,2014-05-23 00:00:00.0,4077,COMPLETE))
(65722,		(164250,65722,730,5,400.0,80.0,		65722,2014-05-23 00:00:00.0,4077,COMPLETE))
------		-------------------------------------------------------------------------------
  _1											_2

(65722,		(164252,65722,627,5,199.95,39.99,  	65722,2014-05-23 00:00:00.0,4077,COMPLETE))
------		---------------------------------	------------------------------------------
_1         				_1										_2
OrderId	       		ORDER_ITEMS  						    	ORDERS	

tuples can be access using ._ and array can be accessed using (1) notation
*/

// <orderDate, totalRevenue>
val revenuePerOrderPerDay = ordersJoinOrderItems.map(t => (t._2._2.split(",")(1), t._2._1.split(",")(4).toFloat))

// revenue per day from joined data
val totalRevenuePerDay = revenuePerOrderPerDay.reduceByKey((t1,t2) => t1+t2)
totalRevenuePerDay.sortByKey().take(10).foreach(println)
/* Output:
(2013-07-25 00:00:00.0,68153.875)                                               
(2013-07-26 00:00:00.0,136520.1)
(2013-07-27 00:00:00.0,101074.33)
(2013-07-28 00:00:00.0,87123.086)
(2013-07-29 00:00:00.0,137287.02)
(2013-07-30 00:00:00.0,102745.59)
(2013-07-31 00:00:00.0,131878.02)
(2013-08-01 00:00:00.0,129001.586)
(2013-08-02 00:00:00.0,109346.99)
(2013-08-03 00:00:00.0,95266.89)   
*/

// Number or orders from order_items on daily basis
// Order count per day
val ordersPerDay = ordersJoinOrderItems.map(t => (t._2._2.split(",")(1) + "," + t._1)).distinct()
val ordersPerDayParsedRDD = ordersPerDay.map(t => (t.split(",")(0), 1))
val totalOrdersPerDay = ordersPerDayParsedRDD.reduceByKey((x,y) => x+y)
totalOrdersPerDay.sortByKey().take(10).foreach(println)
/* Output:
(2013-07-25 00:00:00.0,116)                                                     
(2013-07-26 00:00:00.0,233)
(2013-07-27 00:00:00.0,175)
(2013-07-28 00:00:00.0,158)
(2013-07-29 00:00:00.0,216)
(2013-07-30 00:00:00.0,182)
(2013-07-31 00:00:00.0,209)
(2013-08-01 00:00:00.0,212)
(2013-08-02 00:00:00.0,186)
(2013-08-03 00:00:00.0,159)
*/
 
// Join order count per day and revenue per day
val finalJoinRDD = totalOrdersPerDay.join(totalRevenuePerDay)

finalJoinRDD.sortByKey().take(10).foreach(println)
/* Outpu:
 (2013-07-25 00:00:00.0,(116,68153.875))                                         
(2013-07-26 00:00:00.0,(233,136520.1))
(2013-07-27 00:00:00.0,(175,101074.33))
(2013-07-28 00:00:00.0,(158,87123.086))
(2013-07-29 00:00:00.0,(216,137287.02))
(2013-07-30 00:00:00.0,(182,102745.59))
(2013-07-31 00:00:00.0,(209,131878.02))
(2013-08-01 00:00:00.0,(212,129001.586))
(2013-08-02 00:00:00.0,(186,109346.99))
(2013-08-03 00:00:00.0,(159,95266.89))
*/


/**
Join data sets using hive and sql
*/

// For HiveContext [used when we have hive tables]
import org.apache.spark.sql.hive.HiveContext
val sqlContext = new HiveContext(sc)

// For SQLContext we also require Row [used to create temp tables]
import org.apache.spark.sql.SQLContext
import org.apache.spark.sql.Row 
val sqlContext = new SQLContext(sc)

//set parameter to limit number of partition, by default it use 200
sqlContext.sql("set spark.sql.shuffel.partitions=10")

sqlContext.sql("select * from orders limit 10").collect().foreach(println);

sqlContext.sql("select o.order_date, round(sum(oi.order_item_subtotal), 2), count(distinct(oi.order_item_order_id))
from orders o join order_items oi on o.order_id = oi.order_item_order_id 
group by o.order_date 
order by o.order_date").take(10).foreach(println)


/**
// Execute query agains strutcuted data using SQLContext
// [Read HDFS and create temp tables and execute query against it]
*/

import org.apache.spark.sql.SQLContext
import org.apache.spark.sql.Row 
val sqlContext = new SQLContext(sc)
sqlContext.sql("set spark.sql.shuffel.partitions=10")

//Create Orders
case class Orders(
order_id: Int,
order_date: String,
order_customer_id: Int,
order_status: String
)

val ordersRDD = sc.textFile("/user/cloudera/sqoop_import/orders").
map(o => o.split(",")).
map(o => Orders(o(0).toInt, o(1), o(2).toInt, o(3))).toDF()

// required to register temp tables
import sqlContext.implicits._
ordersRDD.registerTempTable("orders")

// verify records
sqlContext.sql("select * from orders limit 10").collect().foreach(println)

// Create OrderItems
case class OrderItems(
order_item_id: Int,
order_item_order_id: Int,
order_item_product_id: Int,
order_item_quantity: Int,
order_item_subtotal: Float,
order_item_product_price: Float
)

val orderItemsRDD = sc.textFile("/user/cloudera/sqoop_import/order_items").
map(rec => rec.split(",")).
map(oi => OrderItems(oi(0).toInt, oi(1).toInt, oi(2).toInt, oi(3).toInt, oi(4).toFloat, oi(5).toFloat)).toDF()

orderItemsRDD.registerTempTable("order_items")
  
val joinAggData = sqlContext.sql("select o.order_date, round(sum(oi.order_item_subtotal), 2), " +
"count(distinct o.order_id) from orders o join order_items oi " +
"on o.order_id = oi.order_item_order_id " +
"group by o.order_date order by o.order_date")

sqlContext.sql("set spark.sql.shuffel.partitions=10")
joinAggData.collect().foreach(println)


/**
Aggregating Data (avg/sum/max)
*/

/*  Table structure

mysql> describe orders;
+-------------------+-------------+------+-----+---------+----------------+
| Field             | Type        | Null | Key | Default | Extra          |
+-------------------+-------------+------+-----+---------+----------------+
| order_id          | int(11)     | NO   | PRI | NULL    | auto_increment |
| order_date        | datetime    | NO   |     | NULL    |                |
| order_customer_id | int(11)     | NO   |     | NULL    |                |
| order_status      | varchar(45) | NO   |     | NULL    |                |
+-------------------+-------------+------+-----+---------+----------------+

mysql> describe order_items;
+--------------------------+------------+------+-----+---------+----------------+
| Field                    | Type       | Null | Key | Default | Extra          |
+--------------------------+------------+------+-----+---------+----------------+
| order_item_id            | int(11)    | NO   | PRI | NULL    | auto_increment |
| order_item_order_id      | int(11)    | NO   |     | NULL    |                |
| order_item_product_id    | int(11)    | NO   |     | NULL    |                |
| order_item_quantity      | tinyint(4) | NO   |     | NULL    |                |
| order_item_subtotal      | float      | NO   |     | NULL    |                |
| order_item_product_price | float      | NO   |     | NULL    |                |
+--------------------------+------------+------+-----+---------+----------------+
*/

// Total revenue 
val orderItemsRDD = sc.textFile("/user/cloudera/sqoop_import/order_items")
val totalRevenue = orderItemsRDD.map(o => o.split(",")(4).toFloat).reduce((a,v) => a+v)
printf("%f", totalRevenue)

// Average[we can not take from Orders as they may have Canceled orders]
// Help: We can durectly use '_.' instead of defining 'rec =>'
val distinctOrderCount = orderItemsRDD.map(_.split(",")(1).toInt).distinct.count()
val average = totalRevenue/distinctOrderCount

// Max
// [Problem Statement: Find order with highest revenue]
var orderItemsMap = orderItemsRDD.map(rec => (rec.split(",")(1).toInt, rec.split(",")(4).toFloat))

// In reduceByKey, anonymous funtion applied to value only and key will be aggregated 
val revenuePerOrder = orderItemsMap.reduceByKey((acc, value)=> acc+value)

/*  revenuePerOrder: Output
(32676,719.91)
(53926,219.97)
(4926,939.85)

acc._1 = 32676
acc._2 = 719.91 
*/
val maxRevenue = revenuePerOrder.reduce((acc,value) =>
(if(acc._2 >= value._2) acc else value)
)

/* maxRevenue: Output
maxRevenue: (Int, Float) = (68703,3449.91)

Cross verify in MySql
Select order_item_order_id, sum(order_item_subtotal) from order_items
group by order_item_order_id
order by 2;

Output:
68703 |         3449.90998840332
*/

/**
Aggregating Data by key
*/

// Count of orders for each status using countByKey
val ordersRDD = sc.textFile("/user/cloudera/sqoop_import/orders")
ordersRDD.map(rec => (rec.split(",")(3), rec)).countByKey().foreach(println)

/* Validate in sql
select order_status, count(1) from orders group by order_status;
*/

// Count of orders for each status using groupByKey
// Help: groupByKey is not usng combiner internally
val ordersMap = ordersRDD.map(rec => (rec.split(",")(3), 1))
ordersMap.groupByKey().map(rec => (rec._1, rec._2.sum)).collect.foreach(println)

// Count of orders for each status using reduceByKey
// Help: reduceByKey uses combiner internally
ordersMap.reduceByKey((acc, value) => acc+value).collect.foreach(println)

// Count of orders for each status using combineByKey
// Help: both reduceByKey and combineByKey expects type of input and output date are same
val ordersMap = ordersRDD.map(rec => (rec.split(",")(3), 0))
val ordersByStatus = ordersMap.combineByKey(value => 1, (acc: Int, value: Int) => acc+1, (acc: Int, value: Int) => acc+value)

// Count of orders for each status using aggregateByKey
val ordersByStatus = ordersMap.aggregateByKey(0)((acc, value) => acc+1, (acc, value) => acc+value)

/**
Customer ID for max revenue for each day
*/

val ordersRDD = sc.textFile("/user/cloudera/sqoop_import/orders")
val orderItemsRDD = sc.textFile("/user/cloudera/sqoop_import/order_items")

val ordersParsedRDD = ordersRDD.map(rec => (rec.split(",")(0), rec))
val orderItemsParsedRDD = orderItemsRDD.map(rec => (rec.split(",")(1), rec))

val ordersJoinOrderItems = orderItemsParsedRDD.join(ordersParsedRDD)
/* Output
(2828,(		7097,2828,403,1,129.99,129.99,		2828,2013-08-10 00:00:00.0,4952,SUSPECTED_FRAUD))
(43399,(	108397,43399,502,2,100.0,50.0,		43399,2014-04-20 00:00:00.0,16,COMPLETE))
(43399,(	108398,43399,403,1,129.99,129.99,	43399,2014-04-20 00:00:00.0,16,COMPLETE))
(43399,(	108399,43399,1014,1,49.98,49.98,	43399,2014-04-20 00:00:00.0,16,COMPLETE))
(8989,(		22422,8989,627,3,119.97,39.99,		8989,2013-09-19 00:00:00.0,2501,PENDING_PAYMENT))
*/

val ordersPerDayPerCustomer = ordersJoinOrderItems.map(rec => ((rec._2._2.split(",")(1),rec._2._2.split(",")(2)), rec._2._1.split(",")(4).toFloat))
/* Output
((2013-08-10 00:00:00.0,4952),	129.99)                                           
((2014-04-20 00:00:00.0,16),	100.0)
((2014-04-20 00:00:00.0,16),	129.99)
((2014-04-20 00:00:00.0,16),	49.98)
((2013-09-19 00:00:00.0,2501),	119.97)
*/

val revenuePerDayPerCustomer = ordersPerDayPerCustomer.reduceByKey((x, y) => x+y)
/* Output
((2013-08-01 00:00:00.0,5806),	639.88)                                           
((2014-04-25 00:00:00.0,6415),	609.83997)
((2013-10-14 00:00:00.0,10165),	499.95)
((2014-02-20 00:00:00.0,5878),	1199.95)
((2013-10-07 00:00:00.0,9206),	549.97)
*/

val revenuePerDayPerCustomerMap = revenuePerDayPerCustomer.map(rec => (rec._1._1, (rec._1._2, rec._2)))
/* Output
(2013-08-01 00:00:00.0,	(5806,		639.88))                                           
(2014-04-25 00:00:00.0,	(6415,		609.83997))
(2013-10-14 00:00:00.0,	(10165,		499.95))
(2014-02-20 00:00:00.0,	(5878,		1199.95))
(2013-10-07 00:00:00.0,	(9206,		549.97))
*/

val topCustomerPerDayByRevenue = revenuePerDayPerCustomerMap.reduceByKey((x,y) => (if(x._2 >= y._2) x else y)) 
/*Output
(2013-10-05 00:00:00.0,(6647,1429.92))                                          
(2014-07-17 00:00:00.0,(12202,1449.7101))
(2014-05-06 00:00:00.0,(4987,1749.83))
(2014-05-17 00:00:00.0,(6233,1829.88))
(2014-04-25 00:00:00.0,(1670,1449.77))
*/

//Using regular function
def findMax(x: (String, Float), y: (String, Float)): (String, Float) = {
  if(x._2 >= y._2)
    return x
  else
    return y
}
val topCustomerPerDaybyRevenue = revenuePerDayPerCustomerMap.reduceByKey((x, y) => findMax(x, y))

/**
Using HiveContext, find customer ID for max revenue for each day
*/
import org.apache.spark.sql.hive.HiveContext
val hiveContext = new HiveContext(sc)
hiveContext.sql("set spark.sql.shuffle.partitions=10");

hiveContext.sql("select o.order_date, sum(oi.order_item_subtotal)/count(distinct oi.order_item_order_id) from orders o join order_items oi on o.order_id = oi.order_item_order_id group by o.order_date order by o.order_date").collect().foreach(println)


/**
# Filter data into a smaller dataset using Spark
*/
val ordersRDD = sc.textFile("/user/cloudera/sqoop_import/orders")
ordersRDD.filter(line => line.split(",")(3).equals("COMPLETE")).take(5).foreach(println)
ordersRDD.filter(line => line.split(",")(3).contains("PENDING")).take(5).foreach(println)
ordersRDD.filter(line => line.split(",")(0).toInt > 100).take(5).foreach(println)
ordersRDD.filter(line => line.split(",")(0).toInt > 100 || line.split(",")(3).contains("PENDING")).take(5).foreach(println)
ordersRDD.filter(line => line.split(",")(0).toInt > 1000 && 
    (line.split(",")(3).contains("PENDING") || line.split(",")(3).equals("CANCELLED"))).
    take(5).
    foreach(println)
ordersRDD.filter(line => line.split(",")(0).toInt > 1000 && 
    !line.split(",")(3).equals("COMPLETE")).
    take(5).
    foreach(println)

// Check if there are any cancelled orders with amount greater than 1000$
// Get only cancelled orders
// Join orders and order items
// Generate sum(order_item_subtotal) per order
// Filter data which amount to greater than 1000$

val ordersRDD = sc.textFile("/user/cloudera/sqoop_import/orders")
val orderItemsRDD = sc.textFile("/user/cloudera/sqoop_import/order_items")

val ordersParsedRDD = ordersRDD.filter(rec => rec.split(",")(3).contains("CANCELED")).
  map(rec => (rec.split(",")(0).toInt, rec))
val orderItemsParsedRDD = orderItemsRDD.
  map(rec => (rec.split(",")(1).toInt, rec.split(",")(4).toFloat))
val orderItemsAgg = orderItemsParsedRDD.reduceByKey((acc, value) => (acc + value))

val ordersJoinOrderItems = orderItemsAgg.join(ordersParsedRDD)

ordersJoinOrderItems.filter(rec => rec._2._1 >= 1000).take(5).foreach(println)

//Using SQL
import org.apache.spark.sql.hive.HiveContext
val sqlContext = new HiveContext(sc)

sqlContext.sql("select * from (select o.order_id, sum(oi.order_item_subtotal) as order_item_revenue from orders o join order_items oi on o.order_id = oi.order_item_order_id where o.order_status = 'CANCELED' group by o.order_id) q where order_item_revenue >= 1000").count()


/**
Global sorting and ranking (sortByKey, top and takeOrdered)
*/

/**
+---------------------+--------------+------+-----+---------+----------------+
| Field               | Type         | Null | Key | Default | Extra          |
+---------------------+--------------+------+-----+---------+----------------+
| product_id          | int(11)      | NO   | PRI | NULL    | auto_increment |
| product_category_id | int(11)      | NO   |     | NULL    |                |
| product_name        | varchar(45)  | NO   |     | NULL    |                |
| product_description | varchar(255) | NO   |     | NULL    |                |
| product_price       | float        | NO   |     | NULL    |                |
| product_image       | varchar(255) | NO   |     | NULL    |                |
+---------------------+--------------+------+-----+---------+----------------+
*/

// Global sorting by price on Product
val productsRDD = sc.textFile("/user/cloudera/sqoop_import/products")

// key will be colum on which we want sorting, in this case it is product_price
productsRDD.map(rec => (rec.split(",")(4).toFloat, rec)).sortByKey().collect().foreach(println)

/*Output
(999.99,695,32,Callaway Women's Solaire Gems 20-Piece Comple,,999.99,http://images.acmesports.sports/Callaway+Women%27s+Solaire+Gems+20-Piece+Complete+Set+-...)
(1099.99,1048,47,Spalding Beast 60" Glass Portable Basketball ,,1099.99,http://images.acmesports.sports/Spalding+Beast+60%22+Glass+Portable+Basketball+Hoop)
(1799.99,66,4,SOLE F85 Treadmill,,1799.99,http://images.acmesports.sports/SOLE+F85+Treadmill)
(1799.99,199,10,SOLE F85 Treadmill,,1799.99,http://images.acmesports.sports/SOLE+F85+Treadmill)
*/

// Sort in descending order 'sortByKey(false)'
productsRDD.map(rec => (rec.split(",")(4).toFloat, rec)).sortByKey(false).collect().foreach(println)

/*Output
(5.0,336,15,Nike Swoosh Headband - 2",,5.0,http://images.acmesports.sports/Nike+Swoosh+Headband+-+2%22)
(4.99,624,29,adidas Batting Helmet Hardware Kit,,4.99,http://images.acmesports.sports/adidas+Batting+Helmet+Hardware+Kit)
(4.99,815,37,Zero Friction Practice Golf Balls - 12 Pack,,4.99,http://images.acmesports.sports/Zero+Friction+Practice+Golf+Balls+-+12+Pack)
(0.0,38,3,Nike Men's Hypervenom Phantom Premium FG Socc,,0.0,http://images.acmesports.sports/Nike+Men%27s+Hypervenom+Phantom+Premium+FG+Soccer+Cleat)
(0.0,388,18,Nike Men's Hypervenom Phantom Premium FG Socc,,0.0,http://images.acmesports.sports/Nike+Men%27s+Hypervenom+Phantom+Premium+FG+Soccer+Cleat)
*/

// for ranking we can use 'top' action
// top: sort data in ascending order and fetch 5 records
productsRDD.map(rec => (rec.split(",")(4).toFloat, rec)).top(5).foreach(println)

/*Output
(1999.99,208,10,SOLE E35 Elliptical,,1999.99,http://images.acmesports.sports/SOLE+E35+Elliptical)
(1799.99,66,4,SOLE F85 Treadmill,,1799.99,http://images.acmesports.sports/SOLE+F85+Treadmill)
(1799.99,496,22,SOLE F85 Treadmill,,1799.99,http://images.acmesports.sports/SOLE+F85+Treadmill)
(1799.99,199,10,SOLE F85 Treadmill,,1799.99,http://images.acmesports.sports/SOLE+F85+Treadmill)
(1099.99,1048,47,Spalding Beast 60" Glass Portable Basketball ,,1099.99,http://images.acmesports.sports/Spalding+Beast+60%22+Glass+Portable+Basketball+Hoop)
*/

// takeOrdered is same like map+sortByKey+take
//Ordering[Float] is Scala class for order functionality
productsRDD.takeOrdered(5)(Ordering[Float].reverse.on(x => x.split(",")(4).toFloat)).foreach(println)

/*Output
208,10,SOLE E35 Elliptical,,1999.99,http://images.acmesports.sports/SOLE+E35+Elliptical
496,22,SOLE F85 Treadmill,,1799.99,http://images.acmesports.sports/SOLE+F85+Treadmill
199,10,SOLE F85 Treadmill,,1799.99,http://images.acmesports.sports/SOLE+F85+Treadmill
66,4,SOLE F85 Treadmill,,1799.99,http://images.acmesports.sports/SOLE+F85+Treadmill
1048,47,Spalding Beast 60" Glass Portable Basketball ,,1099.99,http://images.acmesports.sports/Spalding+Beast+60%22+Glass+Portable+Basketball+Hoop
*/

/**
Sorting and Ranking by key using groupByKey
Problem: Sort product price per category
*/

/*	Table: Products
+---------------------+--------------+------+-----+---------+----------------+
| Field               | Type         | Null | Key | Default | Extra          |
+---------------------+--------------+------+-----+---------+----------------+
| product_id          | int(11)      | NO   | PRI | NULL    | auto_increment |
| product_category_id | int(11)      | NO   |     | NULL    |                |
| product_name        | varchar(45)  | NO   |     | NULL    |                |
| product_description | varchar(255) | NO   |     | NULL    |                |
| product_price       | float        | NO   |     | NULL    |                |
| product_image       | varchar(255) | NO   |     | NULL    |                |
+---------------------+--------------+------+-----+---------+----------------+

Table: Categories
+------------------------+-------------+------+-----+---------+----------------+
| Field                  | Type        | Null | Key | Default | Extra          |
+------------------------+-------------+------+-----+---------+----------------+
| category_id            | int(11)     | NO   | PRI | NULL    | auto_increment |
| category_department_id | int(11)     | NO   |     | NULL    |                |
| category_name          | varchar(45) | NO   |     | NULL    |                |
+------------------------+-------------+------+-----+---------+----------------+

*/

val productsRDD = sc.textFile("/user/cloudera/sqoop_import/products")
val productsMap = productsRDD.map(rec => (rec.split(",")(1), rec))
val productsGropBy = productsMap.groupByKey()

// to print records with same category
productsGropBy.flatMap(rec => rec._2).take(100).foreach(println)

// Product price sort by Map
productsGropBy.map(rec => (rec._1, rec._2.toList.sortBy(k => k.split(",")(4).toFloat))).take(5).foreach(println)

// Product price sort by flatMap in desending order [flatMap straight apply to value]
productsGropBy.flatMap(rec => (rec._2.toList.sortBy(k => -k.split(",")(4).toFloat))).take(5).foreach(println)

// Using method
//scala> productsGropBy
//Op: res1: org.apache.spark.rdd.RDD[(String, Iterable[String])] = ShuffledRDD[3] at groupByKey at <console>:31

def sortList(rec: (String, Iterable[String])): Iterable[String] = {
   return rec._2.toList.sortBy(k => k.split(",")(4).toFloat)
}
// OP: sortList: (rec: (String, Iterable[String]))Iterable[String]

// using method, [Help: _  is used as 'rec =>']
productsGropBy.flatMap(sortList(_)).take(100).foreach(println)

// Top N products
def getTopN(rec: (String, Iterable[String], topN: Int)): Iterable[String] = {
  return rec._2.toList.sortBy(-_.split(",")(4).toFloat).take(topN)
}
productsGropBy.flatMap(getTopN(_,2)).take(100).foreach(println)

