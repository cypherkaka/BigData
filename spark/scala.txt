/**
Spark using Scala context
**/

/*
Build jar file using sbt
*/
download sbt and set environment variable
/home/cloudera/sbt/bin

//Append following two line to end of nano ~/.bash_profile
SBT_HOME=/home/cloudera/sbt    
export PATH=$PATH:$SBT_HOME/bin

// Run following command to reload bash_profile
[cloudera@quickstart ~]$ . ~/.bash_profile 

// Submit spark job
spark-submit --class "SimpleApp" \
--master local \
/home/cloudera/scala-ex1/target/scala-2.10/simple-project_2.10-1.0.jar

/**
Read data from HDFS and write it back to HDFS
**/
val dataRDD = sc.textFile("/user/cloudera/sqoop_import/departments")
val dataRDD = sc.textFile("hdfs://quickstart.cloudera:8022/user/cloudera/sqoop_import/departments")
dataRDD.count()

// To print each records
dataRDD.collect.foreach(println)

// Store data back to hdfs
dataRDD.saveAsTextFile("hdfs://quickstart.cloudera:8022/user/cloudera/spark/departments")
dataRDD.saveAsObjectFile("hdfs://quickstart.cloudera:8022/user/cloudera/spark/departmentsObject")


/*
Reading and saving sequence file
*/
// Import below package witch include all key types like NullWritable.class 
import org.apache.hadoop.io._

val dataRDD = sc.textFile("/user/cloudera/sqoop_import/departments")
dataRDD.collect.foreach(println)

dataRDD.map(rec => (NullWritable.get(), rec)).saveAsSequenceFile("/user/cloudera/scalaspark/departmentsSeq")
dataRDD.map(rec => (rec.split(",")(0), rec.split(",")(1))).saveAsSequenceFile("/user/cloudera/scalaspark/departmentsSeq")

// reading sequence file with NullWritable as key
sc.sequenceFile("/user/cloudera/scalaspark/departmentsSeq", classOf[NullWritable], classOf[Text]).map(rec => rec.toString()).collect().foreach(println)

// reading sequence file with Text as key
sc.sequenceFile("/user/cloudera/scalaspark/departmentsSeq", classOf[Text], classOf[Text]).map(rec => rec.toString()).collect().foreach(println)

/*
Reading and saving JSon from SQLContext
*/
import org.apache.spark.sql.hive.HiveContext
val sqlContext = new HiveContext(sc)
val departmentsRDD = sqlContext.sql("select * from departments")
departmentsRDD.collect().foreach(println)

// copy following departments.json to hdfs
/*
{"department_id":1, "department_name":"A"}
{"department_id":2, "department_name":"B"}
{"department_id":3, "department_name":"C"}
{"department_id":4, "department_name":"D"}
*/

// copy to hdfs
hadoop fs -put departments.json /user/cloudera/scalaspark

// register table from Json
import org.apache.spark.sql.SQLContext
val sqlContext = new SQLContext(sc)
val departmentsJson = sqlContext.jsonFile("/user/cloudera/scalaspark/departments.json")
departmentsJson.registerTempTable("departmentsTable")
val departmentsData = sqlContext.sql("select * from departmentsTable")
departmentsData.collect().foreach(println)

// Writing data in json format
departmentsData.toJSON.saveAsTextFile("/user/cloudera/scalaspark/departmentsJson")

/**
Word count using flatMap, map, reduceByKey
*/
val data = sc.textFile("/user/cloudera/wordcount.txt") 
val dataFlatMap = data.flatMap(rec => rec.split(" "))
val dataMap = dataFlatMap.map(rec => (rec,1))  
val dataReduceByKey = dataMap.reduceByKey((acc, value) => acc + value)
dataReduceByKey.saveAsTextFile("/user/cloudera/wordcountoutput")

/**
Joining data sets: only requrement is 'common key' between two transformation
*/

//Check database tables to join [orders, order_items]
[cloudera@quickstart ~]$ mysql -u retail_dba -p
mysql> show databases
mysql> use retail_db
mysql> show tables

// Problem: Get revenue and number of orders from order_items on daily basis
val ordersRDD = sc.textFile("/user/cloudera/sqoop_import/orders")
val orderItemsRDD = sc.textFile("/user/cloudera/sqoop_import/order_items")

val ordersParsedRDD = ordersRDD.map(rec => (rec.split(",")(0).toInt, rec))
val orderItemsParsedRDD = orderItemsRDD.map(rec => (rec.split(",")(1).toInt, rec))

val ordersJoinOrderItems = orderItemsParsedRDD.join(ordersParsedRDD)
/*  Output:
scala> ordersJoinOrderItems.take(5).foreach(println)
(41234,(102921,41234,249,2,109.94,54.97,41234,2014-04-04 00:00:00.0,3182,PENDING_PAYMENT))
(65722,(164249,65722,365,2,119.98,59.99,65722,2014-05-23 00:00:00.0,4077,COMPLETE))
(65722,		(164250,65722,730,5,400.0,80.0,		65722,2014-05-23 00:00:00.0,4077,COMPLETE))
------		-------------------------------------------------------------------------------
  _1											_2

(65722,		(164252,65722,627,5,199.95,39.99,  	65722,2014-05-23 00:00:00.0,4077,COMPLETE))
------		---------------------------------	------------------------------------------
_1         				_1										_2
OrderId	       		ORDER_ITEMS  						    	ORDERS	

tuples can be access using ._ and array can be accessed using (1) notation
*/

// <orderDate, totalRevenue>
val revenuePerOrderPerDay = ordersJoinOrderItems.map(t => (t._2._2.split(",")(1), t._2._1.split(",")(4).toFloat))

// revenue per day from joined data
val totalRevenuePerDay = revenuePerOrderPerDay.reduceByKey((t1,t2) => t1+t2)
totalRevenuePerDay.sortByKey().take(10).foreach(println)
/* Output:
(2013-07-25 00:00:00.0,68153.875)                                               
(2013-07-26 00:00:00.0,136520.1)
(2013-07-27 00:00:00.0,101074.33)
(2013-07-28 00:00:00.0,87123.086)
(2013-07-29 00:00:00.0,137287.02)
(2013-07-30 00:00:00.0,102745.59)
(2013-07-31 00:00:00.0,131878.02)
(2013-08-01 00:00:00.0,129001.586)
(2013-08-02 00:00:00.0,109346.99)
(2013-08-03 00:00:00.0,95266.89)   
*/

// Number or orders from order_items on daily basis
// Order count per day
val ordersPerDay = ordersJoinOrderItems.map(t => (t._2._2.split(",")(1) + "," + t._1)).distinct()
val ordersPerDayParsedRDD = ordersPerDay.map(t => (t.split(",")(0), 1))
val totalOrdersPerDay = ordersPerDayParsedRDD.reduceByKey((x,y) => x+y)
totalOrdersPerDay.sortByKey().take(10).foreach(println)
/* Output:
(2013-07-25 00:00:00.0,116)                                                     
(2013-07-26 00:00:00.0,233)
(2013-07-27 00:00:00.0,175)
(2013-07-28 00:00:00.0,158)
(2013-07-29 00:00:00.0,216)
(2013-07-30 00:00:00.0,182)
(2013-07-31 00:00:00.0,209)
(2013-08-01 00:00:00.0,212)
(2013-08-02 00:00:00.0,186)
(2013-08-03 00:00:00.0,159)
*/
 
// Join order count per day and revenue per day
val finalJoinRDD = totalOrdersPerDay.join(totalRevenuePerDay)

finalJoinRDD.sortByKey().take(10).foreach(println)
/* Outpu:
 (2013-07-25 00:00:00.0,(116,68153.875))                                         
(2013-07-26 00:00:00.0,(233,136520.1))
(2013-07-27 00:00:00.0,(175,101074.33))
(2013-07-28 00:00:00.0,(158,87123.086))
(2013-07-29 00:00:00.0,(216,137287.02))
(2013-07-30 00:00:00.0,(182,102745.59))
(2013-07-31 00:00:00.0,(209,131878.02))
(2013-08-01 00:00:00.0,(212,129001.586))
(2013-08-02 00:00:00.0,(186,109346.99))
(2013-08-03 00:00:00.0,(159,95266.89))
*/


/**
Join data sets using hive and sql
*/

// For HiveContext [used when we have hive tables]
import org.apache.spark.sql.hive.HiveContext
val sqlContext = new HiveContext(sc)

// For SQLContext we also require Row [used to create temp tables]
import org.apache.spark.sql.SQLContext
import org.apache.spark.sql.Row 
val sqlContext = new SQLContext(sc)

//set parameter to limit number of partition, by default it use 200
sqlContext.sql("set spark.sql.shuffel.partitions=10")

sqlContext.sql("select * from orders limit 10").collect().foreach(println);

sqlContext.sql("select o.order_date, round(sum(oi.order_item_subtotal), 2), count(distinct(oi.order_item_order_id))
from orders o join order_items oi on o.order_id = oi.order_item_order_id 
group by o.order_date 
order by o.order_date").take(10).foreach(println)


/**
// Execute query agains strutcuted data using SQLContext
// [Read HDFS and create temp tables and execute query against it]
*/

import org.apache.spark.sql.SQLContext
import org.apache.spark.sql.Row 
val sqlContext = new SQLContext(sc)
sqlContext.sql("set spark.sql.shuffel.partitions=10")

//Create Orders
case class Orders(
order_id: Int,
order_date: String,
order_customer_id: Int,
order_status: String
)

val ordersRDD = sc.textFile("/user/cloudera/sqoop_import/orders").
map(o => o.split(",")).
map(o => Orders(o(0).toInt, o(1), o(2).toInt, o(3))).toDF()

// required to register temp tables
import sqlContext.implicits._
ordersRDD.registerTempTable("orders")

// verify records
sqlContext.sql("select * from orders limit 10").collect().foreach(println)

// Create OrderItems
case class OrderItems(
order_item_id: Int,
order_item_order_id: Int,
order_item_product_id: Int,
order_item_quantity: Int,
order_item_subtotal: Float,
order_item_product_price: Float
)

val orderItemsRDD = sc.textFile("/user/cloudera/sqoop_import/order_items").
map(rec => rec.split(",")).
map(oi => OrderItems(oi(0).toInt, oi(1).toInt, oi(2).toInt, oi(3).toInt, oi(4).toFloat, oi(5).toFloat)).toDF()

orderItemsRDD.registerTempTable("order_items")
  
val joinAggData = sqlContext.sql("select o.order_date, round(sum(oi.order_item_subtotal), 2), " +
"count(distinct o.order_id) from orders o join order_items oi " +
"on o.order_id = oi.order_item_order_id " +
"group by o.order_date order by o.order_date")

sqlContext.sql("set spark.sql.shuffel.partitions=10")
joinAggData.collect().foreach(println)


/**
Aggregating Data (avg/sum/max)
*/

/*  Table structure

mysql> describe orders;
+-------------------+-------------+------+-----+---------+----------------+
| Field             | Type        | Null | Key | Default | Extra          |
+-------------------+-------------+------+-----+---------+----------------+
| order_id          | int(11)     | NO   | PRI | NULL    | auto_increment |
| order_date        | datetime    | NO   |     | NULL    |                |
| order_customer_id | int(11)     | NO   |     | NULL    |                |
| order_status      | varchar(45) | NO   |     | NULL    |                |
+-------------------+-------------+------+-----+---------+----------------+

mysql> describe order_items;
+--------------------------+------------+------+-----+---------+----------------+
| Field                    | Type       | Null | Key | Default | Extra          |
+--------------------------+------------+------+-----+---------+----------------+
| order_item_id            | int(11)    | NO   | PRI | NULL    | auto_increment |
| order_item_order_id      | int(11)    | NO   |     | NULL    |                |
| order_item_product_id    | int(11)    | NO   |     | NULL    |                |
| order_item_quantity      | tinyint(4) | NO   |     | NULL    |                |
| order_item_subtotal      | float      | NO   |     | NULL    |                |
| order_item_product_price | float      | NO   |     | NULL    |                |
+--------------------------+------------+------+-----+---------+----------------+
*/

// Total revenue 
val orderItemsRDD = sc.textFile("/user/cloudera/sqoop_import/order_items")
val totalRevenue = orderItemsRDD.map(o => o.split(",")(4).toFloat).reduce((a,v) => a+v)
printf("%f", totalRevenue)

// Average[we can not take from Orders as they may have Canceled orders]
// Help: We can durectly use '_.' instead of defining 'rec =>'
val distinctOrderCount = orderItemsRDD.map(_.split(",")(1).toInt).distinct.count()
val average = totalRevenue/distinctOrderCount

// Max
// [Problem Statement: Find order with highest revenue]
var orderItemsMap = orderItemsRDD.map(rec => (rec.split(",")(1).toInt, rec.split(",")(4).toFloat))

// In reduceByKey, anonymous funtion applied to value only and key will be aggregated 
val revenuePerOrder = orderItemsMap.reduceByKey((acc, value)=> acc+value)

/*  revenuePerOrder: Output
(32676,719.91)
(53926,219.97)
(4926,939.85)

acc._1 = 32676
acc._2 = 719.91 
*/
val maxRevenue = revenuePerOrder.reduce((acc,value) =>
(if(acc._2 >= value._2) acc else value)
)

/* maxRevenue: Output
maxRevenue: (Int, Float) = (68703,3449.91)

Cross verify in MySql
Select order_item_order_id, sum(order_item_subtotal) from order_items
group by order_item_order_id
order by 2;

Output:
68703 |         3449.90998840332
*/

/**
Aggregating Data by key
*/

// Count of orders for each status using countByKey
val ordersRDD = sc.textFile("/user/cloudera/sqoop_import/orders")
ordersRDD.map(rec => (rec.split(",")(3), rec)).countByKey().foreach(println)

/* Validate in sql
select order_status, count(1) from orders group by order_status;
*/

// Count of orders for each status using groupByKey
// Help: groupByKey is not usng combiner internally
val ordersMap = ordersRDD.map(rec => (rec.split(",")(3), 1))
ordersMap.groupByKey().map(rec => (rec._1, rec._2.sum)).collect.foreach(println)

// Count of orders for each status using reduceByKey
// Help: reduceByKey uses combiner internally
ordersMap.reduceByKey((acc, value) => acc+value).collect.foreach(println)

// Count of orders for each status using combineByKey
// Help: both reduceByKey and combineByKey expects type of input and output date are same
val ordersMap = ordersRDD.map(rec => (rec.split(",")(3), 0))
val ordersByStatus = ordersMap.combineByKey(value => 1, (acc: Int, value: Int) => acc+1, (acc: Int, value: Int) => acc+value)

// Count of orders for each status using aggregateByKey
val ordersByStatus = ordersMap.aggregateByKey(0)((acc, value) => acc+1, (acc, value) => acc+value)

/**
Customer ID for max revenue for each day
*/

val ordersRDD = sc.textFile("/user/cloudera/sqoop_import/orders")
val orderItemsRDD = sc.textFile("/user/cloudera/sqoop_import/order_items")

val ordersParsedRDD = ordersRDD.map(rec => (rec.split(",")(0), rec))
val orderItemsParsedRDD = orderItemsRDD.map(rec => (rec.split(",")(1), rec))

val ordersJoinOrderItems = orderItemsParsedRDD.join(ordersParsedRDD)
/* Output
(2828,(		7097,2828,403,1,129.99,129.99,		2828,2013-08-10 00:00:00.0,4952,SUSPECTED_FRAUD))
(43399,(	108397,43399,502,2,100.0,50.0,		43399,2014-04-20 00:00:00.0,16,COMPLETE))
(43399,(	108398,43399,403,1,129.99,129.99,	43399,2014-04-20 00:00:00.0,16,COMPLETE))
(43399,(	108399,43399,1014,1,49.98,49.98,	43399,2014-04-20 00:00:00.0,16,COMPLETE))
(8989,(		22422,8989,627,3,119.97,39.99,		8989,2013-09-19 00:00:00.0,2501,PENDING_PAYMENT))
*/

val ordersPerDayPerCustomer = ordersJoinOrderItems.map(rec => ((rec._2._2.split(",")(1),rec._2._2.split(",")(2)), rec._2._1.split(",")(4).toFloat))
/* Output
((2013-08-10 00:00:00.0,4952),	129.99)                                           
((2014-04-20 00:00:00.0,16),	100.0)
((2014-04-20 00:00:00.0,16),	129.99)
((2014-04-20 00:00:00.0,16),	49.98)
((2013-09-19 00:00:00.0,2501),	119.97)
*/

val revenuePerDayPerCustomer = ordersPerDayPerCustomer.reduceByKey((x, y) => x+y)
/* Output
((2013-08-01 00:00:00.0,5806),	639.88)                                           
((2014-04-25 00:00:00.0,6415),	609.83997)
((2013-10-14 00:00:00.0,10165),	499.95)
((2014-02-20 00:00:00.0,5878),	1199.95)
((2013-10-07 00:00:00.0,9206),	549.97)
*/

val revenuePerDayPerCustomerMap = revenuePerDayPerCustomer.map(rec => (rec._1._1, (rec._1._2, rec._2)))
/* Output
(2013-08-01 00:00:00.0,	(5806,		639.88))                                           
(2014-04-25 00:00:00.0,	(6415,		609.83997))
(2013-10-14 00:00:00.0,	(10165,		499.95))
(2014-02-20 00:00:00.0,	(5878,		1199.95))
(2013-10-07 00:00:00.0,	(9206,		549.97))
*/

val topCustomerPerDayByRevenue = revenuePerDayPerCustomerMap.reduceByKey((x,y) => (if(x._2 >= y._2) x else y)) 
/*Output
(2013-10-05 00:00:00.0,(6647,1429.92))                                          
(2014-07-17 00:00:00.0,(12202,1449.7101))
(2014-05-06 00:00:00.0,(4987,1749.83))
(2014-05-17 00:00:00.0,(6233,1829.88))
(2014-04-25 00:00:00.0,(1670,1449.77))
*/

//Using regular function
def findMax(x: (String, Float), y: (String, Float)): (String, Float) = {
  if(x._2 >= y._2)
    return x
  else
    return y
}
val topCustomerPerDaybyRevenue = revenuePerDayPerCustomerMap.reduceByKey((x, y) => findMax(x, y))

/**
Using HiveContext, find customer ID for max revenue for each day
*/
import org.apache.spark.sql.hive.HiveContext
val hiveContext = new HiveContext(sc)
hiveContext.sql("set spark.sql.shuffle.partitions=10");

hiveContext.sql("select o.order_date, sum(oi.order_item_subtotal)/count(distinct oi.order_item_order_id) from orders o join order_items oi on o.order_id = oi.order_item_order_id group by o.order_date order by o.order_date").collect().foreach(println)







