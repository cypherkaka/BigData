/**
Spark using Scala context
**/

/****  Basic Scala  **** 

scala> val list = List(3,4,9,0,1,2,0,8,3,4,1,2,6,4,9,0,9)
list: List[Int] = List(3, 4, 9, 0, 1, 2, 0, 8, 3, 4, 1, 2, 6, 4, 9, 0, 9)

scala> val list: List[Int] = List(3,4,9,0,1,2,0,8,3,4,1,2,6,4,9,0,9)
list: List[Int] = List(3, 4, 9, 0, 1, 2, 0, 8, 3, 4, 1, 2, 6, 4, 9, 0, 9)

scala> list.length
res0: Int = 17

scala> list.distinct
res2: List[Int] = List(3, 4, 9, 0, 1, 2, 8, 6)

scala> list.sorted
res3: List[Int] = List(0, 0, 0, 1, 1, 2, 2, 3, 3, 4, 4, 4, 6, 8, 9, 9, 9)

scala> list.reverse
res4: List[Int] = List(9, 0, 9, 4, 6, 2, 1, 4, 3, 8, 0, 2, 1, 0, 9, 4, 3)

scala> list.sorted.reverse
res5: List[Int] = List(9, 9, 9, 8, 6, 4, 4, 4, 3, 3, 2, 2, 1, 1, 0, 0, 0)

scala> list.distinct.sorted.reverse
res6: List[Int] = List(9, 8, 6, 4, 3, 2, 1, 0)

scala> list.distinct.sorted.reverse.take(4)
res7: List[Int] = List(9, 8, 6, 4)

scala> list(1)
res8: Int = 4

scala> //to add all elements

scala> list.reduce(_ + _)
res9: Int = 65

scala> val t:(Int, Int)=(2,8)
t: (Int, Int) = (2,8)

scala> t._1
res10: Int = 2

scala> t._2
res11: Int = 8

scala> val t: (Int, List[Int]) = (1, List(3,8,1,1,7,8))
t: (Int, List[Int]) = (1,List(3, 8, 1, 1, 7, 8))

scala> t._2
res12: List[Int] = List(3, 8, 1, 1, 7, 8)

scala> t._2(4)
res13: Int = 7

scala> val t: (Int, (List[Int], List[String])) = (1, (List(2,9,0,2,6,2,0), List("Hello", "World")))
t: (Int, (List[Int], List[String])) = (1,(List(2, 9, 0, 2, 6, 2, 0),List(Hello, World)))

scala> t._1
res14: Int = 1

scala> t._2
res15: (List[Int], List[String]) = (List(2, 9, 0, 2, 6, 2, 0),List(Hello, World))

scala> t._2._2
res16: List[String] = List(Hello, World)

scala> t._2._2(1)
res17: String = World
*/

/*
Start spark shell with arguments
*/
spark-shell --master yarn-client --conf spark.ui.port=23123

/*
Build jar file using sbt
*/
download sbt and set environment variable
/home/cloudera/sbt/bin

//Append following two line to end of nano ~/.bash_profile
SBT_HOME=/home/cloudera/sbt    
export PATH=$PATH:$SBT_HOME/bin

// Run following command to reload bash_profile
[cloudera@quickstart ~]$ . ~/.bash_profile 

// Submit spark job
spark-submit --class "SimpleApp" \
--master local \
/home/cloudera/scala-ex1/target/scala-2.10/simple-project_2.10-1.0.jar

/**
Read data from HDFS and write it back to HDFS
**/
val dataRDD = sc.textFile("/user/cloudera/sqoop_import/departments")
val dataRDD = sc.textFile("hdfs://quickstart.cloudera:8022/user/cloudera/sqoop_import/departments")
dataRDD.count()

// To print each records
dataRDD.collect.foreach(println)

// Store data back to hdfs
dataRDD.saveAsTextFile("hdfs://quickstart.cloudera:8022/user/cloudera/spark/departments")
dataRDD.saveAsObjectFile("hdfs://quickstart.cloudera:8022/user/cloudera/spark/departmentsObject")


/*
Reading and saving sequence file
*/
// Import below package witch include all key types like NullWritable.class 
import org.apache.hadoop.io._

val dataRDD = sc.textFile("/user/cloudera/sqoop_import/departments")
dataRDD.collect.foreach(println)

dataRDD.map(rec => (NullWritable.get(), rec)).saveAsSequenceFile("/user/cloudera/scalaspark/departmentsSeq")
dataRDD.map(rec => (rec.split(",")(0), rec.split(",")(1))).saveAsSequenceFile("/user/cloudera/scalaspark/departmentsSeq")

// reading sequence file with NullWritable as key
sc.sequenceFile("/user/cloudera/scalaspark/departmentsSeq", classOf[NullWritable], classOf[Text]).map(rec => rec.toString()).collect().foreach(println)

// reading sequence file with Text as key
sc.sequenceFile("/user/cloudera/scalaspark/departmentsSeq", classOf[Text], classOf[Text]).map(rec => rec.toString()).collect().foreach(println)

/*
Reading and saving JSon from SQLContext
*/
import org.apache.spark.sql.hive.HiveContext   // No need to import, it is available
val sqlContext = new HiveContext(sc)	// No need to create sqlContext
val departmentsRDD = sqlContext.sql("select * from departments")
departmentsRDD.collect().foreach(println)

// copy following departments.json to hdfs
/*
{"department_id":1, "department_name":"A"}
{"department_id":2, "department_name":"B"}
{"department_id":3, "department_name":"C"}
{"department_id":4, "department_name":"D"}
*/

// copy to hdfs
hadoop fs -put departments.json /user/cloudera/scalaspark

// register table from Json
import org.apache.spark.sql.SQLContext
val sqlContext = new SQLContext(sc)
val departmentsJson = sqlContext.jsonFile("/user/cloudera/scalaspark/departments.json")
departmentsJson.registerTempTable("departmentsTable")
val departmentsData = sqlContext.sql("select * from departmentsTable")
departmentsData.collect().foreach(println)

// Writing data in json format
departmentsData.toJSON.saveAsTextFile("/user/cloudera/scalaspark/departmentsJson")

/**
Word count using flatMap, map, reduceByKey
*/
val data = sc.textFile("/user/cloudera/wordcount.txt") 
val dataFlatMap = data.flatMap(rec => rec.split(" "))
val dataMap = dataFlatMap.map(rec => (rec,1))  
val dataReduceByKey = dataMap.reduceByKey((acc, value) => acc + value)
dataReduceByKey.saveAsTextFile("/user/cloudera/wordcountoutput")

/**
Joining data sets: only requrement is 'common key' between two transformation
*/

//Check database tables to join [orders, order_items]
[cloudera@quickstart ~]$ mysql -u retail_dba -p
mysql> show databases
mysql> use retail_db
mysql> show tables

// Problem: Get revenue and number of orders from order_items on daily basis
val ordersRDD = sc.textFile("/user/cloudera/sqoop_import/orders")
val orderItemsRDD = sc.textFile("/user/cloudera/sqoop_import/order_items")

val ordersParsedRDD = ordersRDD.map(rec => (rec.split(",")(0).toInt, rec))
val orderItemsParsedRDD = orderItemsRDD.map(rec => (rec.split(",")(1).toInt, rec))

val ordersJoinOrderItems = orderItemsParsedRDD.join(ordersParsedRDD)
/*  Output:
scala> ordersJoinOrderItems.take(5).foreach(println)
(41234,(102921,41234,249,2,109.94,54.97,41234,2014-04-04 00:00:00.0,3182,PENDING_PAYMENT))
(65722,(164249,65722,365,2,119.98,59.99,65722,2014-05-23 00:00:00.0,4077,COMPLETE))
(65722,		(164250,65722,730,5,400.0,80.0,		65722,2014-05-23 00:00:00.0,4077,COMPLETE))
------		-------------------------------------------------------------------------------
  _1											_2

(65722,		(164252,65722,627,5,199.95,39.99,  	65722,2014-05-23 00:00:00.0,4077,COMPLETE))
------		---------------------------------	------------------------------------------
_1         				_1										_2
OrderId	       		ORDER_ITEMS  						    	ORDERS	

tuples can be access using ._ and array can be accessed using (1) notation
*/

// <orderDate, totalRevenue>
val revenuePerOrderPerDay = ordersJoinOrderItems.map(t => (t._2._2.split(",")(1), t._2._1.split(",")(4).toFloat))

// revenue per day from joined data
val totalRevenuePerDay = revenuePerOrderPerDay.reduceByKey((t1,t2) => t1+t2)
totalRevenuePerDay.sortByKey().take(10).foreach(println)
/* Output:
(2013-07-25 00:00:00.0,68153.875)                                               
(2013-07-26 00:00:00.0,136520.1)
(2013-07-27 00:00:00.0,101074.33)
(2013-07-28 00:00:00.0,87123.086)
(2013-07-29 00:00:00.0,137287.02)
(2013-07-30 00:00:00.0,102745.59)
(2013-07-31 00:00:00.0,131878.02)
(2013-08-01 00:00:00.0,129001.586)
(2013-08-02 00:00:00.0,109346.99)
(2013-08-03 00:00:00.0,95266.89)   
*/

// Number or orders from order_items on daily basis
// Order count per day
val ordersPerDay = ordersJoinOrderItems.map(t => (t._2._2.split(",")(1) + "," + t._1)).distinct()
val ordersPerDayParsedRDD = ordersPerDay.map(t => (t.split(",")(0), 1))
val totalOrdersPerDay = ordersPerDayParsedRDD.reduceByKey((x,y) => x+y)
totalOrdersPerDay.sortByKey().take(10).foreach(println)
/* Output:
(2013-07-25 00:00:00.0,116)                                                     
(2013-07-26 00:00:00.0,233)
(2013-07-27 00:00:00.0,175)
(2013-07-28 00:00:00.0,158)
(2013-07-29 00:00:00.0,216)
(2013-07-30 00:00:00.0,182)
(2013-07-31 00:00:00.0,209)
(2013-08-01 00:00:00.0,212)
(2013-08-02 00:00:00.0,186)
(2013-08-03 00:00:00.0,159)
*/
 
// Join order count per day and revenue per day
val finalJoinRDD = totalOrdersPerDay.join(totalRevenuePerDay)

finalJoinRDD.sortByKey().take(10).foreach(println)
/* Outpu:
 (2013-07-25 00:00:00.0,(116,68153.875))                                         
(2013-07-26 00:00:00.0,(233,136520.1))
(2013-07-27 00:00:00.0,(175,101074.33))
(2013-07-28 00:00:00.0,(158,87123.086))
(2013-07-29 00:00:00.0,(216,137287.02))
(2013-07-30 00:00:00.0,(182,102745.59))
(2013-07-31 00:00:00.0,(209,131878.02))
(2013-08-01 00:00:00.0,(212,129001.586))
(2013-08-02 00:00:00.0,(186,109346.99))
(2013-08-03 00:00:00.0,(159,95266.89))
*/


/**
Join data sets using hive and sql
*/

// For HiveContext [used when we have hive tables]
import org.apache.spark.sql.hive.HiveContext  // not required, as org.apache.spark.sql.SQLContext = org.apache.spark.sql.hive.HiveContext@6bd028f2
val sqlContext = new HiveContext(sc)  // Not required

// For SQLContext we also require Row [used to create temp tables]
import org.apache.spark.sql.SQLContext
import org.apache.spark.sql.Row 
val sqlContext = new SQLContext(sc)

//set parameter to limit number of partition, by default it use 200
sqlContext.sql("set spark.sql.shuffel.partitions=10")

sqlContext.sql("select * from orders limit 10").collect().foreach(println);

sqlContext.sql("select o.order_date, round(sum(oi.order_item_subtotal), 2), count(distinct(oi.order_item_order_id))
from orders o join order_items oi on o.order_id = oi.order_item_order_id 
group by o.order_date 
order by o.order_date").take(10).foreach(println)

// NOTE:  Above approach is lengthy and short approach is available
val productsRDD = sc.textFile("/user/cloudera/sqoop_import/products")
//productsRDD: org.apache.spark.rdd.RDD[String] = /user/cloudera/sqoop_import/products MapPartitionsRDD[10] at textFile at <console>:27

val productsDF = productsRDD.toDF
//productsDF: org.apache.spark.sql.DataFrame = [_1: string]

sqlContext.sql("set spark.sql.shuffel.partitions=10")
//res12: org.apache.spark.sql.DataFrame = [key: string, value: string]

sqlContext.sql("select * from products_temp limit 4").collect.foreach(println)
//[1,2,Quest Q64 10 FT. x 10 FT. Slant Leg Instant U,,59.98,http://images.acmesports.sports/Quest+Q64+10+FT.+x+10+FT.+Slant+Leg+Instant+Up+Canopy]
//[2,2,Under Armour Men's Highlight MC Football Clea,,129.99,http://images.acmesports.sports/Under+Armour+Men%27s+Highlight+MC+Football+Cleat]
//[3,2,Under Armour Men's Renegade D Mid Football Cl,,89.99,http://images.acmesports.sports/Under+Armour+Men%27s+Renegade+D+Mid+Football+Cleat]
//[4,2,Under Armour Men's Renegade D Mid Football Cl,,89.99,http://images.acmesports.sports/Under+Armour+Men%27s+Renegade+D+Mid+Football+Cleat]


/**
// Execute query agains strutcuted data using SQLContext
// [Read HDFS and create temp tables and execute query against it]
*/

import org.apache.spark.sql.SQLContext
import org.apache.spark.sql.Row 
val sqlContext = new SQLContext(sc)
sqlContext.sql("set spark.sql.shuffel.partitions=10")

//Create Orders
case class Orders(
order_id: Int,
order_date: String,
order_customer_id: Int,
order_status: String
)

val ordersRDD = sc.textFile("/user/cloudera/sqoop_import/orders").
map(o => o.split(",")).
map(o => Orders(o(0).toInt, o(1), o(2).toInt, o(3))).toDF()

// required to register temp tables
import sqlContext.implicits._
ordersRDD.registerTempTable("orders")

// verify records
sqlContext.sql("select * from orders limit 10").collect().foreach(println)

// Create OrderItems
case class OrderItems(
order_item_id: Int,
order_item_order_id: Int,
order_item_product_id: Int,
order_item_quantity: Int,
order_item_subtotal: Float,
order_item_product_price: Float
)

val orderItemsRDD = sc.textFile("/user/cloudera/sqoop_import/order_items").
map(rec => rec.split(",")).
map(oi => OrderItems(oi(0).toInt, oi(1).toInt, oi(2).toInt, oi(3).toInt, oi(4).toFloat, oi(5).toFloat)).toDF()

orderItemsRDD.registerTempTable("order_items")
  
val joinAggData = sqlContext.sql("select o.order_date, round(sum(oi.order_item_subtotal), 2), " +
"count(distinct o.order_id) from orders o join order_items oi " +
"on o.order_id = oi.order_item_order_id " +
"group by o.order_date order by o.order_date")

sqlContext.sql("set spark.sql.shuffel.partitions=10")
joinAggData.collect().foreach(println)


/**
Aggregating Data (avg/sum/max)
*/

/*  Table structure

mysql> describe orders;
+-------------------+-------------+------+-----+---------+----------------+
| Field             | Type        | Null | Key | Default | Extra          |
+-------------------+-------------+------+-----+---------+----------------+
| order_id          | int(11)     | NO   | PRI | NULL    | auto_increment |
| order_date        | datetime    | NO   |     | NULL    |                |
| order_customer_id | int(11)     | NO   |     | NULL    |                |
| order_status      | varchar(45) | NO   |     | NULL    |                |
+-------------------+-------------+------+-----+---------+----------------+

mysql> describe order_items;
+--------------------------+------------+------+-----+---------+----------------+
| Field                    | Type       | Null | Key | Default | Extra          |
+--------------------------+------------+------+-----+---------+----------------+
| order_item_id            | int(11)    | NO   | PRI | NULL    | auto_increment |
| order_item_order_id      | int(11)    | NO   |     | NULL    |                |
| order_item_product_id    | int(11)    | NO   |     | NULL    |                |
| order_item_quantity      | tinyint(4) | NO   |     | NULL    |                |
| order_item_subtotal      | float      | NO   |     | NULL    |                |
| order_item_product_price | float      | NO   |     | NULL    |                |
+--------------------------+------------+------+-----+---------+----------------+
*/

// Total revenue 
val orderItemsRDD = sc.textFile("/user/cloudera/sqoop_import/order_items")
val totalRevenue = orderItemsRDD.map(o => o.split(",")(4).toFloat).reduce((a,v) => a+v)
BigDecimal(totalRevenue)
//res15: scala.math.BigDecimal = 34326256

printf("%f", totalRevenue)
//34326256.000000

// Average[we can not take from Orders as they may have Canceled orders]
// Help: We can durectly use '_.' instead of defining 'rec =>'
val distinctOrderCount = orderItemsRDD.map(_.split(",")(1).toInt).distinct.count()
val average = totalRevenue/distinctOrderCount

// Max
// [Problem Statement: Find order with highest revenue]
var orderItemsMap = orderItemsRDD.map(rec => (rec.split(",")(1).toInt, rec.split(",")(4).toFloat))

// In reduceByKey, anonymous funtion applied to value only and key will be aggregated 
val revenuePerOrder = orderItemsMap.reduceByKey((acc, value)=> acc+value)

/*  revenuePerOrder: Output
(32676,719.91)
(53926,219.97)
(4926,939.85)

acc._1 = 32676
acc._2 = 719.91 
*/
val maxRevenue = revenuePerOrder.reduce((acc,value) =>
(if(acc._2 >= value._2) acc else value)
)

/* maxRevenue: Output
maxRevenue: (Int, Float) = (68703,3449.91)

Cross verify in MySql
Select order_item_order_id, sum(order_item_subtotal) from order_items
group by order_item_order_id
order by 2;

Output:
68703 |         3449.90998840332
*/

/**
Aggregating Data by key
*/

// Count of orders for each status using countByKey
val ordersRDD = sc.textFile("/user/cloudera/sqoop_import/orders")
ordersRDD.map(rec => (rec.split(",")(3), 1)).countByKey.foreach(println)
/*
(PAYMENT_REVIEW,729)                                                            
(CLOSED,7556)
(SUSPECTED_FRAUD,1558)
(PROCESSING,8275)
(COMPLETE,22899)
(PENDING,7610)
(PENDING_PAYMENT,15030)
(ON_HOLD,3798)
(CANCELED,1428)
*/

/* Validate in sql
select order_status, count(1) from orders group by order_status;
*/

// Count of orders for each status using groupByKey
// Help: groupByKey is not usng combiner internally
val ordersMap = ordersRDD.map(rec => (rec.split(",")(3), 1))
ordersMap.groupByKey().map(rec => (rec._1, rec._2.sum)).collect.foreach(println)

// Count of orders for each status using reduceByKey
// Help: reduceByKey uses combiner internally
ordersMap.reduceByKey((acc, value) => acc+value).collect.foreach(println)

// Count of orders for each status using combineByKey
// Help: both reduceByKey and combineByKey expects type of input and output date are same
val ordersMap = ordersRDD.map(rec => (rec.split(",")(3), 0))
val ordersByStatus = ordersMap.combineByKey(value => 1, (acc: Int, value: Int) => acc+1, (acc: Int, value: Int) => acc+value)

// Count of orders for each status using aggregateByKey
val ordersByStatus = ordersMap.aggregateByKey(0)((acc, value) => acc+1, (acc, value) => acc+value)

/**
Customer ID for max revenue for each day
*/

val ordersRDD = sc.textFile("/user/cloudera/sqoop_import/orders")
val orderItemsRDD = sc.textFile("/user/cloudera/sqoop_import/order_items")

val ordersParsedRDD = ordersRDD.map(rec => (rec.split(",")(0), rec))
val orderItemsParsedRDD = orderItemsRDD.map(rec => (rec.split(",")(1), rec))

val ordersJoinOrderItems = orderItemsParsedRDD.join(ordersParsedRDD)
/* Output
(2828,(		7097,2828,403,1,129.99,129.99,		2828,2013-08-10 00:00:00.0,4952,SUSPECTED_FRAUD))
(43399,(	108397,43399,502,2,100.0,50.0,		43399,2014-04-20 00:00:00.0,16,COMPLETE))
(43399,(	108398,43399,403,1,129.99,129.99,	43399,2014-04-20 00:00:00.0,16,COMPLETE))
(43399,(	108399,43399,1014,1,49.98,49.98,	43399,2014-04-20 00:00:00.0,16,COMPLETE))
(8989,(		22422,8989,627,3,119.97,39.99,		8989,2013-09-19 00:00:00.0,2501,PENDING_PAYMENT))
*/

val ordersPerDayPerCustomer = ordersJoinOrderItems.map(rec => ((rec._2._2.split(",")(1),rec._2._2.split(",")(2)), rec._2._1.split(",")(4).toFloat))
/* Output
((2013-08-10 00:00:00.0,4952),	129.99)                                           
((2014-04-20 00:00:00.0,16),	100.0)
((2014-04-20 00:00:00.0,16),	129.99)
((2014-04-20 00:00:00.0,16),	49.98)
((2013-09-19 00:00:00.0,2501),	119.97)
*/

val revenuePerDayPerCustomer = ordersPerDayPerCustomer.reduceByKey((x, y) => x+y)
/* Output
((2013-08-01 00:00:00.0,5806),	639.88)                                           
((2014-04-25 00:00:00.0,6415),	609.83997)
((2013-10-14 00:00:00.0,10165),	499.95)
((2014-02-20 00:00:00.0,5878),	1199.95)
((2013-10-07 00:00:00.0,9206),	549.97)
*/

val revenuePerDayPerCustomerMap = revenuePerDayPerCustomer.map(rec => (rec._1._1, (rec._1._2, rec._2)))
/* Output
(2013-08-01 00:00:00.0,	(5806,		639.88))                                           
(2014-04-25 00:00:00.0,	(6415,		609.83997))
(2013-10-14 00:00:00.0,	(10165,		499.95))
(2014-02-20 00:00:00.0,	(5878,		1199.95))
(2013-10-07 00:00:00.0,	(9206,		549.97))
*/

val topCustomerPerDayByRevenue = revenuePerDayPerCustomerMap.reduceByKey((x,y) => (if(x._2 >= y._2) x else y)) 
/*Output
(2013-10-05 00:00:00.0,(6647,1429.92))                                          
(2014-07-17 00:00:00.0,(12202,1449.7101))
(2014-05-06 00:00:00.0,(4987,1749.83))
(2014-05-17 00:00:00.0,(6233,1829.88))
(2014-04-25 00:00:00.0,(1670,1449.77))
*/

//Using regular function
def findMax(x: (String, Float), y: (String, Float)): (String, Float) = {
  if(x._2 >= y._2)
    return x
  else
    return y
}
val topCustomerPerDaybyRevenue = revenuePerDayPerCustomerMap.reduceByKey((x, y) => findMax(x, y))

/**
Using HiveContext, find customer ID for max revenue for each day
*/
import org.apache.spark.sql.hive.HiveContext
val hiveContext = new HiveContext(sc)
hiveContext.sql("set spark.sql.shuffle.partitions=10");

hiveContext.sql("select o.order_date, sum(oi.order_item_subtotal)/count(distinct oi.order_item_order_id) from orders o join order_items oi on o.order_id = oi.order_item_order_id group by o.order_date order by o.order_date").collect().foreach(println)


/**
# Filter data into a smaller dataset using Spark
*/
val ordersRDD = sc.textFile("/user/cloudera/sqoop_import/orders")
ordersRDD.filter(line => line.split(",")(3).equals("COMPLETE")).take(5).foreach(println)
ordersRDD.filter(line => line.split(",")(3).contains("PENDING")).take(5).foreach(println)
ordersRDD.filter(line => line.split(",")(0).toInt > 100).take(5).foreach(println)
ordersRDD.filter(line => line.split(",")(0).toInt > 100 || line.split(",")(3).contains("PENDING")).take(5).foreach(println)
ordersRDD.filter(line => line.split(",")(0).toInt > 1000 && 
    (line.split(",")(3).contains("PENDING") || line.split(",")(3).equals("CANCELLED"))).
    take(5).
    foreach(println)
ordersRDD.filter(line => line.split(",")(0).toInt > 1000 && 
    !line.split(",")(3).equals("COMPLETE")).
    take(5).
    foreach(println)

// Check if there are any cancelled orders with amount greater than 1000$
// Get only cancelled orders
// Join orders and order items
// Generate sum(order_item_subtotal) per order
// Filter data which amount to greater than 1000$

val ordersRDD = sc.textFile("/user/cloudera/sqoop_import/orders")
val orderItemsRDD = sc.textFile("/user/cloudera/sqoop_import/order_items")

val ordersParsedRDD = ordersRDD.filter(rec => rec.split(",")(3).contains("CANCELED")).
  map(rec => (rec.split(",")(0).toInt, rec))
val orderItemsParsedRDD = orderItemsRDD.
  map(rec => (rec.split(",")(1).toInt, rec.split(",")(4).toFloat))
val orderItemsAgg = orderItemsParsedRDD.reduceByKey((acc, value) => (acc + value))

val ordersJoinOrderItems = orderItemsAgg.join(ordersParsedRDD)

ordersJoinOrderItems.filter(rec => rec._2._1 >= 1000).take(5).foreach(println)

//Using SQL
import org.apache.spark.sql.hive.HiveContext
val sqlContext = new HiveContext(sc)

sqlContext.sql("select * from (select o.order_id, sum(oi.order_item_subtotal) as order_item_revenue from orders o join order_items oi on o.order_id = oi.order_item_order_id where o.order_status = 'CANCELED' group by o.order_id) q where order_item_revenue >= 1000").count()


/**
Global sorting and ranking (sortByKey, top and takeOrdered)
*/

/**
+---------------------+--------------+------+-----+---------+----------------+
| Field               | Type         | Null | Key | Default | Extra          |
+---------------------+--------------+------+-----+---------+----------------+
| product_id          | int(11)      | NO   | PRI | NULL    | auto_increment |
| product_category_id | int(11)      | NO   |     | NULL    |                |
| product_name        | varchar(45)  | NO   |     | NULL    |                |
| product_description | varchar(255) | NO   |     | NULL    |                |
| product_price       | float        | NO   |     | NULL    |                |
| product_image       | varchar(255) | NO   |     | NULL    |                |
+---------------------+--------------+------+-----+---------+----------------+
*/

// Global sorting by price on Product
val productsRDD = sc.textFile("/user/cloudera/sqoop_import/products")

// key will be colum on which we want sorting, in this case it is product_price
productsRDD.map(rec => (rec.split(",")(4).toFloat, rec)).sortByKey().collect().foreach(println)

/*Output
(999.99,695,32,Callaway Women's Solaire Gems 20-Piece Comple,,999.99,http://images.acmesports.sports/Callaway+Women%27s+Solaire+Gems+20-Piece+Complete+Set+-...)
(1099.99,1048,47,Spalding Beast 60" Glass Portable Basketball ,,1099.99,http://images.acmesports.sports/Spalding+Beast+60%22+Glass+Portable+Basketball+Hoop)
(1799.99,66,4,SOLE F85 Treadmill,,1799.99,http://images.acmesports.sports/SOLE+F85+Treadmill)
(1799.99,199,10,SOLE F85 Treadmill,,1799.99,http://images.acmesports.sports/SOLE+F85+Treadmill)
*/

// Sort in descending order 'sortByKey(false)'
productsRDD.map(rec => (rec.split(",")(4).toFloat, rec)).sortByKey(false).collect().foreach(println)

/*Output
(5.0,336,15,Nike Swoosh Headband - 2",,5.0,http://images.acmesports.sports/Nike+Swoosh+Headband+-+2%22)
(4.99,624,29,adidas Batting Helmet Hardware Kit,,4.99,http://images.acmesports.sports/adidas+Batting+Helmet+Hardware+Kit)
(4.99,815,37,Zero Friction Practice Golf Balls - 12 Pack,,4.99,http://images.acmesports.sports/Zero+Friction+Practice+Golf+Balls+-+12+Pack)
(0.0,38,3,Nike Men's Hypervenom Phantom Premium FG Socc,,0.0,http://images.acmesports.sports/Nike+Men%27s+Hypervenom+Phantom+Premium+FG+Soccer+Cleat)
(0.0,388,18,Nike Men's Hypervenom Phantom Premium FG Socc,,0.0,http://images.acmesports.sports/Nike+Men%27s+Hypervenom+Phantom+Premium+FG+Soccer+Cleat)
*/

// for ranking we can use 'top' action
// top: sort data in ascending order and fetch 5 records
productsRDD.map(rec => (rec.split(",")(4).toFloat, rec)).top(5).foreach(println)

/*Output
(1999.99,208,10,SOLE E35 Elliptical,,1999.99,http://images.acmesports.sports/SOLE+E35+Elliptical)
(1799.99,66,4,SOLE F85 Treadmill,,1799.99,http://images.acmesports.sports/SOLE+F85+Treadmill)
(1799.99,496,22,SOLE F85 Treadmill,,1799.99,http://images.acmesports.sports/SOLE+F85+Treadmill)
(1799.99,199,10,SOLE F85 Treadmill,,1799.99,http://images.acmesports.sports/SOLE+F85+Treadmill)
(1099.99,1048,47,Spalding Beast 60" Glass Portable Basketball ,,1099.99,http://images.acmesports.sports/Spalding+Beast+60%22+Glass+Portable+Basketball+Hoop)
*/

// takeOrdered is same like map+sortByKey+take
//Ordering[Float] is Scala class for order functionality
productsRDD.takeOrdered(5)(Ordering[Float].reverse.on(x => x.split(",")(4).toFloat)).foreach(println)

/*Output
208,10,SOLE E35 Elliptical,,1999.99,http://images.acmesports.sports/SOLE+E35+Elliptical
496,22,SOLE F85 Treadmill,,1799.99,http://images.acmesports.sports/SOLE+F85+Treadmill
199,10,SOLE F85 Treadmill,,1799.99,http://images.acmesports.sports/SOLE+F85+Treadmill
66,4,SOLE F85 Treadmill,,1799.99,http://images.acmesports.sports/SOLE+F85+Treadmill
1048,47,Spalding Beast 60" Glass Portable Basketball ,,1099.99,http://images.acmesports.sports/Spalding+Beast+60%22+Glass+Portable+Basketball+Hoop
*/

/**
Sorting and Ranking by key using groupByKey
Problem: Sort product price per category
*/

/*	Table: Products
+---------------------+--------------+------+-----+---------+----------------+
| Field               | Type         | Null | Key | Default | Extra          |
+---------------------+--------------+------+-----+---------+----------------+
| product_id          | int(11)      | NO   | PRI | NULL    | auto_increment |
| product_category_id | int(11)      | NO   |     | NULL    |                |
| product_name        | varchar(45)  | NO   |     | NULL    |                |
| product_description | varchar(255) | NO   |     | NULL    |                |
| product_price       | float        | NO   |     | NULL    |                |
| product_image       | varchar(255) | NO   |     | NULL    |                |
+---------------------+--------------+------+-----+---------+----------------+

Table: Categories
+------------------------+-------------+------+-----+---------+----------------+
| Field                  | Type        | Null | Key | Default | Extra          |
+------------------------+-------------+------+-----+---------+----------------+
| category_id            | int(11)     | NO   | PRI | NULL    | auto_increment |
| category_department_id | int(11)     | NO   |     | NULL    |                |
| category_name          | varchar(45) | NO   |     | NULL    |                |
+------------------------+-------------+------+-----+---------+----------------+

*/

val productsRDD = sc.textFile("/user/cloudera/sqoop_import/products")
val productsMap = productsRDD.map(rec => (rec.split(",")(1), rec))
val productsGropBy = productsMap.groupByKey()

// to print records with same category
productsGropBy.flatMap(rec => rec._2).take(100).foreach(println)

// Product price sort by Map
productsGropBy.map(rec => (rec._1, rec._2.toList.sortBy(k => k.split(",")(4).toFloat))).take(5).foreach(println)

// Product price sort by flatMap in desending order [flatMap straight apply to value]
productsGropBy.flatMap(rec => (rec._2.toList.sortBy(k => -k.split(",")(4).toFloat))).take(5).foreach(println)

// Using method
//scala> productsGropBy
//Op: res1: org.apache.spark.rdd.RDD[(String, Iterable[String])] = ShuffledRDD[3] at groupByKey at <console>:31

def sortList(rec: (String, Iterable[String])): Iterable[String] = {
   return rec._2.toList.sortBy(k => k.split(",")(4).toFloat)
}
// OP: sortList: (rec: (String, Iterable[String]))Iterable[String]

// using method, [Help: _  is used as 'rec =>']
productsGropBy.flatMap(sortList(_)).take(100).foreach(println)

// Top N products
def getTopN(rec: (String, Iterable[String], topN: Int)): Iterable[String] = {
  return rec._2.toList.sortBy(-_.split(",")(4).toFloat).take(topN)
}
productsGropBy.flatMap(getTopN(_,2)).take(100).foreach(println)



/**
* DATA FRAMES
*/

// Define Case Class 
// [By default all variables are immutable and has public get/set methods]
case class Orders(
  order_id: Int,
  order_date: String,
  order_customer_id: Int,
  order_status: String
);

// Define normal class [Normal class with constructor]
class Ords(
  order_id: Int,
  order_date: String,
  order_customer_id: Int,
  order_status: String
);

// Create DataFram for Orders
val ordersDFTmp = sc.textFile("/user/cloudera/sqoop_import/orders").
  map(rec => {
  	val r = rec.split(",")
  	Orders(r(0).toInt, r(1), r(2).toInt, r(3))
  }).toDF

/* O/P
scala> val ordersDF = sc.textFile("/user/cloudera/sqoop_import/orders").
     |   map(rec => {
     |   val r = rec.split(",")
     |   Orders(r(0).toInt, r(1), r(2).toInt, r(3))
     |   }).toDF
ordersDF: org.apache.spark.sql.DataFrame = [order_id: int, order_date: string, order_customer_id: int, order_status: string]
*/

// select few columns from dataframe
ordersDF.select("order_id", "order_date").take(100).foreach(println)

/*
scala> ordersDF.select("order_id", "order_date").take(4).foreach(println)
[1,2013-07-25 00:00:00.0]
[2,2013-07-25 00:00:00.0]
[3,2013-07-25 00:00:00.0]
[4,2013-07-25 00:00:00.0]

scala> ordersDF.select("*").take(4).foreach(println)
[1,2013-07-25 00:00:00.0,11599,CLOSED]
[2,2013-07-25 00:00:00.0,256,PENDING_PAYMENT]
[3,2013-07-25 00:00:00.0,12111,COMPLETE]
[4,2013-07-25 00:00:00.0,8827,CLOSED]
*/


/*---------------
* EX 2: Create Case classes for Orders,OrderItems and execute DataFrame operations 
*/

case class Orders(
  order_id: Int,
  order_date: String,
  order_customer_id: Int,
  order_status: String
);

case class OrderItems(
  order_item_id: Int,
  order_item_order_id: Int,
  order_item_product_id: Int,
  order_item_quantity: Int,
  order_item_subtotal: Double,
  order_item_product_price: Double
);

val orders = sc.textFile("/user/cloudera/sqoop_import/orders");
val orderItems = sc.textFile("/user/cloudera/sqoop_import/order_items");

/* OP
orders.first()
res1: String = 1,2013-07-25 00:00:00.0,11599,CLOSED
*/

// Create DataFrame for Orders and OrderItems
val ordersDF = orders.map(rec => {
  val r = rec.split(",")
  Orders(r(0).toInt, r(1), r(2).toInt, r(3))
}).toDF

val orderItemsDF = orderItems.map(rec => {
  val r = rec.split(",")
  OrderItems(r(0).toInt, r(1).toInt, r(2).toInt, r(3).toInt, r(4).toDouble, r(5).toDouble)
}).toDF

/* Execute Data Frame operations 

//Verify schema
scala> ordersDF.printSchema
root
 |-- order_id: integer (nullable = false)
 |-- order_date: string (nullable = true)
 |-- order_customer_id: integer (nullable = false)
 |-- order_status: string (nullable = true)


scala> orderItemsDF.printSchema
root
 |-- order_item_id: integer (nullable = false)
 |-- order_item_order_id: integer (nullable = false)
 |-- order_item_product_id: integer (nullable = false)
 |-- order_item_quantity: integer (nullable = false)
 |-- order_item_subtotal: double (nullable = false)
 |-- order_item_product_price: double (nullable = false)

// to view sample data
scala> ordersDF.show
+--------+--------------------+-----------------+---------------+
|order_id|          order_date|order_customer_id|   order_status|
+--------+--------------------+-----------------+---------------+
|       1|2013-07-25 00:00:...|            11599|         CLOSED|
|       2|2013-07-25 00:00:...|              256|PENDING_PAYMENT|
|       3|2013-07-25 00:00:...|            12111|       COMPLETE|
|       4|2013-07-25 00:00:...|             8827|         CLOSED|
|       5|2013-07-25 00:00:...|            11318|       COMPLETE|
|       6|2013-07-25 00:00:...|             7130|       COMPLETE|
|       7|2013-07-25 00:00:...|             4530|       COMPLETE|
|       8|2013-07-25 00:00:...|             2911|     PROCESSING|
|       9|2013-07-25 00:00:...|             5657|PENDING_PAYMENT|
|      10|2013-07-25 00:00:...|             5648|PENDING_PAYMENT|
|      11|2013-07-25 00:00:...|              918| PAYMENT_REVIEW|
|      12|2013-07-25 00:00:...|             1837|         CLOSED|
|      13|2013-07-25 00:00:...|             9149|PENDING_PAYMENT|
|      14|2013-07-25 00:00:...|             9842|     PROCESSING|
|      15|2013-07-25 00:00:...|             2568|       COMPLETE|
|      16|2013-07-25 00:00:...|             7276|PENDING_PAYMENT|
|      17|2013-07-25 00:00:...|             2667|       COMPLETE|
|      18|2013-07-25 00:00:...|             1205|         CLOSED|
|      19|2013-07-25 00:00:...|             9488|PENDING_PAYMENT|
|      20|2013-07-25 00:00:...|             9198|     PROCESSING|
+--------+--------------------+-----------------+---------------+
only showing top 20 rows

// Create tmp table and execute sql

scala> ordersDF.registerTempTable("orders")

scala> sqlContext.sql("select * from orders limit 10").collect().foreach(println)
[1,2013-07-25 00:00:00.0,11599,CLOSED]
[2,2013-07-25 00:00:00.0,256,PENDING_PAYMENT]
[3,2013-07-25 00:00:00.0,12111,COMPLETE]
[4,2013-07-25 00:00:00.0,8827,CLOSED]
[5,2013-07-25 00:00:00.0,11318,COMPLETE]
[6,2013-07-25 00:00:00.0,7130,COMPLETE]
[7,2013-07-25 00:00:00.0,4530,COMPLETE]
[8,2013-07-25 00:00:00.0,2911,PROCESSING]
[9,2013-07-25 00:00:00.0,5657,PENDING_PAYMENT]
[10,2013-07-25 00:00:00.0,5648,PENDING_PAYMENT]

// In above query we must use collect() to get results because .sql() is type  of dataframe
scala> sqlContext.sql("select * from orders limit 10")
res11: org.apache.spark.sql.DataFrame = [order_id: int, order_date: string, order_customer_id: int, order_status: string]

// select few fields on DF
scala> ordersDF.select("order_id", "order_date").show()
+--------+--------------------+
|order_id|          order_date|
+--------+--------------------+
|       1|2013-07-25 00:00:...|
|       2|2013-07-25 00:00:...|
|       3|2013-07-25 00:00:...|
|       4|2013-07-25 00:00:...|
|       5|2013-07-25 00:00:...|
|       6|2013-07-25 00:00:...|
|       7|2013-07-25 00:00:...|
|       8|2013-07-25 00:00:...|
|       9|2013-07-25 00:00:...|
|      10|2013-07-25 00:00:...|
|      11|2013-07-25 00:00:...|
|      12|2013-07-25 00:00:...|
|      13|2013-07-25 00:00:...|
|      14|2013-07-25 00:00:...|
|      15|2013-07-25 00:00:...|
|      16|2013-07-25 00:00:...|
|      17|2013-07-25 00:00:...|
|      18|2013-07-25 00:00:...|
|      19|2013-07-25 00:00:...|
|      20|2013-07-25 00:00:...|
+--------+--------------------+
only showing top 20 rows


// Apply filter on Data Frame
scala> val ordersFiltered = ordersDF.filter(ordersDF("order_status") === "COMPLETE")
ordersFiltered: org.apache.spark.sql.DataFrame = [order_id: int, order_date: string, order_customer_id: int, order_status: string]

scala> ordersFiltered.printSchema
root
 |-- order_id: integer (nullable = false)
 |-- order_date: string (nullable = true)
 |-- order_customer_id: integer (nullable = false)
 |-- order_status: string (nullable = true)


scala> ordersFiltered.show
+--------+--------------------+-----------------+------------+
|order_id|          order_date|order_customer_id|order_status|
+--------+--------------------+-----------------+------------+
|       3|2013-07-25 00:00:...|            12111|    COMPLETE|
|       5|2013-07-25 00:00:...|            11318|    COMPLETE|
|       6|2013-07-25 00:00:...|             7130|    COMPLETE|
|       7|2013-07-25 00:00:...|             4530|    COMPLETE|
|      15|2013-07-25 00:00:...|             2568|    COMPLETE|
|      17|2013-07-25 00:00:...|             2667|    COMPLETE|
|      22|2013-07-25 00:00:...|              333|    COMPLETE|
|      26|2013-07-25 00:00:...|             7562|    COMPLETE|
|      28|2013-07-25 00:00:...|              656|    COMPLETE|
|      32|2013-07-25 00:00:...|             3960|    COMPLETE|
|      35|2013-07-25 00:00:...|             4840|    COMPLETE|
|      45|2013-07-25 00:00:...|             2636|    COMPLETE|
|      56|2013-07-25 00:00:...|            10519|    COMPLETE|
|      63|2013-07-25 00:00:...|             1148|    COMPLETE|
|      65|2013-07-25 00:00:...|             5903|    COMPLETE|
|      67|2013-07-25 00:00:...|             1406|    COMPLETE|
|      71|2013-07-25 00:00:...|             8646|    COMPLETE|
|      72|2013-07-25 00:00:...|             4349|    COMPLETE|
|      76|2013-07-25 00:00:...|             6898|    COMPLETE|
|      80|2013-07-25 00:00:...|             3007|    COMPLETE|
+--------+--------------------+-----------------+------------+
only showing top 20 rows


// Filter with or condition 
// [Note: 'OR' will not work as it is case sensitive]
scala> ordersDF.filter(ordersDF("order_status") === "COMPLETE" or ordersDF("order_status") === "CLOSED").show()
+--------+--------------------+-----------------+------------+
|order_id|          order_date|order_customer_id|order_status|
+--------+--------------------+-----------------+------------+
|       1|2013-07-25 00:00:...|            11599|      CLOSED|
|       3|2013-07-25 00:00:...|            12111|    COMPLETE|
|       4|2013-07-25 00:00:...|             8827|      CLOSED|
|       5|2013-07-25 00:00:...|            11318|    COMPLETE|
|       6|2013-07-25 00:00:...|             7130|    COMPLETE|
|       7|2013-07-25 00:00:...|             4530|    COMPLETE|
|      12|2013-07-25 00:00:...|             1837|      CLOSED|
|      15|2013-07-25 00:00:...|             2568|    COMPLETE|
|      17|2013-07-25 00:00:...|             2667|    COMPLETE|
|      18|2013-07-25 00:00:...|             1205|      CLOSED|
|      22|2013-07-25 00:00:...|              333|    COMPLETE|
|      24|2013-07-25 00:00:...|            11441|      CLOSED|
|      25|2013-07-25 00:00:...|             9503|      CLOSED|
|      26|2013-07-25 00:00:...|             7562|    COMPLETE|
|      28|2013-07-25 00:00:...|              656|    COMPLETE|
|      32|2013-07-25 00:00:...|             3960|    COMPLETE|
|      35|2013-07-25 00:00:...|             4840|    COMPLETE|
|      37|2013-07-25 00:00:...|             5863|      CLOSED|
|      45|2013-07-25 00:00:...|             2636|    COMPLETE|
|      51|2013-07-25 00:00:...|            12271|      CLOSED|
+--------+--------------------+-----------------+------------+
only showing top 20 rows

*/

/*
 Find revenue for each day for COMPLETED and CLOSED orders
*/
val ordersFiltered = ordersDF.filter(ordersDF("order_status") === "COMPLETED" or ordersDF("order_status") === "CLOSED")

// Create join 
val ordersJoin = ordersFiltered.join(orderItemsDF, ordersDF("order_id") === orderItemsDF("order_item_order_id"))


// by default spark will use 200 shuffle partitions so we should limit it
// sqlContext.setConf("spark.sql.shuffel.partitions","2")

ordersJoin.groupBy("order_date").
  agg(sum("order_item_subtotal")).
  show()

// If sum() is not available we have to import spark funtions
// import org.apache.spark.sql.functions._

/*
* Spark sql, Find revenue for each day for COMPLETED and CLOSED orders
* we will use orders and order_items which are registered table using .registerTempTable()
*/
sqlContext.sql("select order_date, sum(order_item_subtotal) dsily_revenue from orders "+
 "join order_items on order_item_order_id = order_id "+
 "where order_status in ('COMPLETE', 'CLOSED') "+
 "group by order_date").show()

// if above not work we have to put 's' as prefix
sqlContext.sql(s"select order_date, sum(order_item_subtotal) dsily_revenue from orders "+
 "join order_items on order_item_order_id = order_id "+
 "where order_status in ('COMPLETE', 'CLOSED') "+
 "group by order_date").show()



/*
Create SBT project and package it to jar file
*/

1. Create dir structure
[cloudera@quickstart NetworkWordCount]$ mkdir -p src/main/scala


2. CD to source directory
[cloudera@quickstart NetworkWordCount]$ cd src/main/scala

3. Create scala file
gedit NetworkWordCount.scala
---------------- Content of scala file -------------------
object NetworkWordCount extends App {
    println("NetworkWordCount - Part 1 ")
}
---------------------------------------------------------

4. Compile sbt project using 'sbt compile'
[cloudera@quickstart scala]$ sbt compile
[info] Set current project to scala (in build file:/home/cloudera/NetworkWordCount/src/main/scala/)
[info] Updating {file:/home/cloudera/NetworkWordCount/src/main/scala/}scala...
[info] Resolving org.fusesource.jansi#jansi;1.4 ...
[info] Done updating.
[info] Compiling 1 Scala source to /home/cloudera/NetworkWordCount/src/main/scala/target/scala-2.10/classes...
[info] 'compiler-interface' not yet compiled for Scala 2.10.6. Compiling...
[info]   Compilation completed in 17.09 s

5. Run compiled package using 'sbt run'
[cloudera@quickstart scala]$ sbt run
[info] Set current project to scala (in build file:/home/cloudera/NetworkWordCount/src/main/scala/)
[info] Running NetworkWordCount 
NetworkWordCount - Part 1 
[success] Total time: 1 s, completed May 9, 2017 1:15:26 PM

6. Package project to jar file using 'sbt package'
[cloudera@quickstart scala]$ sbt package
[info] Set current project to scala (in build file:/home/cloudera/NetworkWordCount/src/main/scala/)
[info] Packaging /home/cloudera/NetworkWordCount/src/main/scala/target/scala-2.10/scala_2.10-0.1-SNAPSHOT.jar ...
[info] Done packaging.
[success] Total time: 1 s, completed May 9, 2017 1:16:11 PM


/*
Create sbt structure from scrat
*/
1. Create empty structure of given project product_name

sbt new sbt/scala-seed.g8			

[cloudera@quickstart NetworkWordCount]$ sbt new sbt/scala-seed.g8
[info] Set current project to networkwordcount (in build file:/home/cloudera/NetworkWordCount/)
SLF4J: Failed to load class "org.slf4j.impl.StaticLoggerBinder".
SLF4J: Defaulting to no-operation (NOP) logger implementation
SLF4J: See http://www.slf4j.org/codes.html#StaticLoggerBinder for further details.

Minimum Scala build. 

name [My Something Project]: WordCount     

Template applied in ./wordcount

2. cd to new directory
cd ./wordcount

3. enter in sbt console and execute 'compile' or 'run' or 'package'
[cloudera@quickstart wordcount]$ sbt
[info] Loading project definition from /home/cloudera/NetworkWordCount/wordcount/project
[info] Updating {file:/home/cloudera/NetworkWordCount/wordcount/project/}wordcount-build...
[info] Resolving org.fusesource.jansi#jansi;1.4 ...
[info] Done updating.
[info] Compiling 1 Scala source to /home/cloudera/NetworkWordCount/wordcount/project/target/scala-2.10/sbt-0.13/classes...
[info] Set current project to Hello (in build file:/home/cloudera/NetworkWordCount/wordcount/)
> run
[info] Updating {file:/home/cloudera/NetworkWordCount/wordcount/}root...
[info] Resolving jline#jline;2.14.1 ...
[info] Done updating.
[info] Compiling 1 Scala source to /home/cloudera/NetworkWordCount/wordcount/target/scala-2.12/classes...
[info] 'compiler-interface' not yet compiled for Scala 2.12.1. Compiling...
[info] Resolving org.scala-sbt#interface;0.13.13 ...
[trace] Stack trace suppressed: run last compile:compileIncremental for the full output.
[error] (compile:compileIncremental) java.lang.UnsupportedClassVersionError: scala/tools/nsc/Main : Unsupported major.minor version 52.0
[error] Total time: 1 s, completed May 9, 2017 1:40:16 PM



/*
Create SparkStreamingContext  from exis SparkContext
*/

import org.apache.spark.streaming._

val sc = ...                // existing SparkContext
val ssc = new StreamingContext(sc, Seconds(1))

/*
To create StreamingContext 
- stop existing spark context
- create streaming context
*/
sc.stop()
import org.apache.spark.SparkConf
import org.apache.spark.streaming.StreamingContext
import org.apache.spark.streaming.Seconds
val conf = new SparkConf().setAppName("Spark Streaming")
val ssc = new StreamingContext(conf, Seconds(30))
val lines = ssc.socketTextStream("localhost", 44444)
val words = lines.flatMap(_.split(" "))
val pairs = words.map(word => (word, 1))
val wordCounts = pairs.reduceByKey(_ + _)
wordCounts.print()
ssc.start()
ssc.awaitTermination()
