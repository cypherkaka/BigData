## --------- Spark Details --------- ##

# Run spark shell in Yarn context
spark-shell --master yarn

# Spark with scala context[DEFAULT]
spark-shell

# Spark with python context in YARN mode
pyspark

# Spark with python context in local mode
pyspark --master local

# To Create SQL context with Hive support, Create soft link for hive Context
sudo ln -s /etc/hive/conf/hive-site.xml /etc/spark/conf/hive-site.xml

# After starting spark-shell we can see "Output: SQL context available as sqlContext."

# Execute Sql query in Spark Context
sqlContext.sql("select * from departments").collect().foreach(println)
sqlContext.sql("select * from departments").count()

# To launch SQL Context
from pyspark.sql import SQLContext

# To launch Hive Context
from pyspark.sql import HiveContext

# Execute Sql query in pyspark context
from pyspark.sql import HiveContext
sqlContext = HiveContext(sc)
depts = sqlContext.sql("select * from departments")

# Search mysql connector
sudo find / -name "mysql-connector*.jar" | grep "/usr/share"

# Pyspark: Connect using mysql driver
pyspark --driver-class-path /usr/share/java/mysql-connector-java.jar
from pyspark.sql import SQLContext
sqlContext = SQLContext(sc)
jdbcurl = "jdbc:mysql://quickstart.cloudera:3306/retail_db?user=retail_dba&password=cloudera"
df = sqlContext.load(source="jdbc", url=jdbcurl, dbtable="departments")
for rec in df.collect():
  print(rec)


# Execute Python script from Scala context
# Read a file from HDFS location and save it back at different location using Python
  # 1. Create a file and save as .py extension
from pyspark import SparkContext, SparkConf
conf = SparkConf().setAppName("pyspark")
sc = SparkContext(conf=conf)
dataRDD = sc.textFile("/user/cloudera/sqoop_import/departments")
for line in dataRDD.collect():
    print(line)
dataRDD.saveAsTextFile("/user/cloudera/pyspark/departmentsTesting")

  # 2. Run below Spark command
spark-submit --master yarn saveFile.py


# pyspark: Read local file and print
data = sc.textFile("file:///home/cloudera/orders.java")
for line in data.collect():
  print(line)

# Write and read SequenceFile
dataRDD = sc.textFile("/user/cloudera/sqoop_import/departments")
for line in dataRDD.collect():
  print(line)

dataRDD.map(lambda x: tuple(x.split(",", 1))).saveAsSequenceFile("/user/cloudera/pyspark/departmentsAsSequenceFile")
data = sc.sequenceFile("/user/cloudera/pyspark/departmentsAsSequenceFile")
for record in data.collect():
  print(record)

data = sc.sequenceFile("/user/cloudera/pyspark/departmentsAsSequenceFile", "org.apache.hadoop.io.IntWritable", "org.apache.hadoop.io.Text")
for record in data.collect():
    print(record)

# Read data from HiveContext
from pyspark.sql import HiveContext
sqlContext = HiveContext(sc)
for row in sqlContext.sql("select * from departments").collect():
  print(row)

# Read and write JSon files
from pyspark import SQLContext
sqlContext = SQLContext(sc)
departmentsJson = sqlContext.jsonFile("/user/cloudera/pyspark/departments.json")
departmentsJson.registerTempTable("djson")
for row in sqlContext.sql("select * from djson").collect():
  print(row)

sqlContext.sql("select * from djson").toJSON().saveAsTextFile("/user/cloudera/pyspark/departmentsJson")

# Word count using Transformation[flatMap, map, reduceByKey]
data = sc.textFile("/user/cloudera/wordcount.txt")
dataFlatMap = data.flatMap(lambda x: x.split(" "))
dataMap = dataFlatMap.map(lambda x: (x,1))
wordCount = dataMap.reduceByKey(lambda x,y: x+y)
for i in wordCount.collect():
  print(i)


# Joining Data Sets using Transformation --------------------------------
[Problem: Get the revenue and number of orders from order_items on daily bases]
[Info: [] means array of items and () means individual tuple]

ordersRDD = sc.textFile("/user/cloudera/sqoop_import/orders")
orderItemsRDD = sc.textFile("/user/cloudera/sqoop_import/order_items")

ordersParsedRDD = ordersRDD.map(lambda rec: (int(rec.split(",")[0]), rec))
orderItemsParsedRDD = orderItemsRDD.map(lambda rec: (int(rec.split(",")[0]), rec))

// smallerTable.join(largetTable)
ordersJoinOrderItems = orderItemsParsedRDD.join(ordersParsedRDD)

// (4, (u'4,2,403,1,129.99,129.99', u'4,2013-07-25 00:00:00.0,8827,CLOSED'))
// Create map[date,subTotal]
revenuePerOrderPerDay = ordersJoinOrderItems.map(lambda t: (t[1][1].split(",")[1], float(t[1][0].split(",")[4])))

// Get order count per day
ordersPerDay = ordersJoinOrderItems.map(lambda t: t[1][1].split(",")[1] + "," + str(t[0])).distinct()
ordersPerDayParsedRDD = ordersPerDay.map(lambda t: (t.split(",")[0],1))

/*
reduceByKey: Expect that the input RDD contains tuples of the form (<key>,<value>).
Create a new RDD containing a tuple for each unique value of <key> in the input,
where the value in the second position of the tuple is created by applying the supplied lambda function to the <value>s
with the matching <key> in the input RDD
*/
totalOrdersPerDay = ordersPerDayParsedRDD.reduceByKey(lambda x,y: x+y)
totalRevenuePerDay = revenuePerOrderPerDay.reduceByKey(lambda ord1, ord2: ord1+ord2)
totalOrdersAndTotalRevenuePerDay = totalOrdersPerDay.join(totalRevenuePerDay)

for i in totalOrdersAndTotalRevenuePerDay.sortByKey().collect():
  print i

# ------ ------------ ------------ ----------- ----------- -------------------

# Joining Data Sets using pyspark and HiveContext --------------------------------
from pyspark.sql import HiveContext
sqlContext = HiveContext(sc)
sqlContext.sql("set spark.sql.shuffle.partitions=10")

joinAggData = sqlContext.sql("select o.order_date, sum(oi.order_item_subtotal), count(distinct(o.order_id)) \
from orders o join order_items oi on o.order_id = oi.order_item_order_id \
group by o.order_date \
order by o.order_date")

# Join using spark native sql
from pyspark.sql import SQLContext, Row
sqlContext = SQLContext(sc)
sqlContext.sql("set spark.sql.shuffle.partitions=10");

ordersRDD = sc.textFile("/user/cloudera/sqoop_import/orders")
ordersMap = ordersRDD.map(lambda o: o.split(","))
orders = ordersMap.map(lambda o: Row(order_id=int(o[0]), order_date=o[1], \
order_customer_id=int(o[2]), order_status=o[3]))
ordersSchema = sqlContext.inferSchema(orders)
ordersSchema.registerTempTable("orders")

orderItemsRDD = sc.textFile("/user/cloudera/sqoop_import/order_items")
orderItemsMap = orderItemsRDD.map(lambda oi: oi.split(","))
orderItems = orderItemsMap.map(lambda oi: Row(order_item_id=int(oi[0]), order_item_order_id=int(oi[1]), \
order_item_product_id=int(oi[2]), order_item_quantity=int(oi[3]), order_item_subtotal=float(oi[4]), \
order_item_product_price=float(oi[5])))
orderItemsSchema = sqlContext.inferSchema(orderItems)
orderItemsSchema.registerTempTable("order_items")

joinAggData = sqlContext.sql("select o.order_date, sum(oi.order_item_subtotal), \
count(distinct o.order_id) from orders o join order_items oi \
on o.order_id = oi.order_item_order_id \
group by o.order_date order by o.order_date")

for data in joinAggData.collect(): print data

# Calculate aggregate statistics(avg, sum, min, max)

# Calculate Total revenue
#1 Find column index, which is 4th (order_item_subtotal)
hive> describe order_items;
OK
order_item_id       	int
order_item_order_id 	int
order_item_product_id	int
order_item_quantity 	tinyint
order_item_subtotal 	double
order_item_product_price	double

orderItemsRDD = sc.textFile("/user/cloudera/sqoop_import/order_items")
orderItemsMap = orderItemsRDD.map(lambda rec: float(rec.split(",")[4]))
// reduce() will will do total
orderItemsMap.reduce(lambda rev1, rev2: rev1+rev2)

# Find max price of produce
# Find column index which is 4th(product_price)
hive> describe products;
OK
product_id          	int
product_category_id 	int
product_name        	string
product_description 	string
product_price       	double
product_image       	string

productsRDD = sc.textFile("/user/cloudera/sqoop_import/products")
productsMap = productsRDD.map(lambda rec: rec)
productsMap.reduce(lambda rec1, rec2: (rec1 if(float(rec1.split(",")[4] >= float(rec2.split(",")[4])) else rec2))

---------- Troubleshoot if any record having ',' --------------
[if any record is having ',' in name, we have to delete that recoed from hadoop file and reupload file]
login into mysql and find records
select * from products where product_name like '%,%'
hadoop fs -get /location/of file
nano part***
ctrl+w
ctrl+t
enter line number
delete and save file
re-upload file to hadoop
hadoop fs -put -f <part***> <currenct locatoin>
---------- Troubleshoot if any record having ',' --------------

"""
# Compute average from totalrevenue 
# - calculate total revenue
# - select distinct orders 
""" 
orderItemsRDD = sc.textFile("/user/cloudera/sqoop_import/order_items")
totalRevenue = orderItemsRDD.map(lambda rec: float(rec.split(",")[4])).reduce(lambda val1, val2: val1+val2)
distinctOrders = orderItemsRDD.map(lambda rec: rec.split(",")[1]).distinct().count()
totalRevenue/distinctOrders

"""
# Operations by key: calculate total orders for order status     
# - countByKey() will give unique count for each key (in this case ORDER_STATUS) 
"""
ordersRDD = sc.textFile("/user/cloudera/sqoop_import/orders")
# must provide 0 as tuple and to create map like <'ORDER_STATUS', 0>
ordersMap = ordersRDD.map(lambda rec: (rec.split(",")[3],0))
for i in ordersMap.countByKey().items(): print i


"""
# Get count by using tranformation(4 ways): 
# - 1: groupByKey()  
# - 2: reduceByKey()
# - 3: aggregateByKey()
# - 4: combineByKey()
"""
ordersRDD = sc.textFile("/user/cloudera/sqoop_import/orders")
ordersMap = ordersRDD.map(lambda rec: (rec.split(",")[3],1))
totalOrdersByStatus = ordersMap.groupByKey().map(lambda rec: (rec[0], sum(rec[1]))) 
for i in totalOrdersByStatus.collect(): print i

totalOrdersByStatusWithReduceByKey = ordersMap.reduceByKey(lambda acc, val: acc+val)
for i in totalOrdersByStatusWithReduceByKey.collect(): print i

# aggregateByKey() used for custom combiner and reducer with initial value (in this case 0)
ordersMap = ordersRDD.map(lambda rec: (rec.split(",")[3], rec))
totalOrdersByStatusWithAggregateByKey = ordersMap.aggregateByKey(0, lambda acc, val: acc+1, lambda acc, val: acc+val)
for i in totalOrdersByStatusWithAggregateByKey.collect(): print i

# combineByKey() same as aggregateByKey() but we can also provide custom incremental argument 
totalOrdersByStatusWithCombineByKey = ordersMap.combineByKey(lambda val: 1, lambda acc, val: acc+1, lambda acc, val: acc+val)
for i in totalOrdersByStatusWithCombineByKey.collect(): print i
