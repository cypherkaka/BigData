## --------- Spark Details --------- ##

# Run spark shell in Yarn context
spark-shell --master yarn

# Spark with scala context[DEFAULT]
spark-shell

# Spark with python context in YARN mode
pyspark

# Spark with python context in local mode
pyspark --master local

# To Create SQL context with Hive support, Create soft link for hive Context
sudo ln -s /etc/hive/conf/hive-site.xml /etc/spark/conf/hive-site.xml

# After starting spark-shell we can see "Output: SQL context available as sqlContext."

# Execute Sql query in Spark Context
sqlContext.sql("select * from departments").collect().foreach(println)
sqlContext.sql("select * from departments").count()

# To launch SQL Context
from pyspark.sql import SQLContext

# To launch Hive Context
from pyspark.sql import HiveContext

# Execute Sql query in pyspark context
from pyspark.sql import HiveContext
sqlContext = HiveContext(sc)
depts = sqlContext.sql("select * from departments")

# Search mysql connector
sudo find / -name "mysql-connector*.jar" | grep "/usr/share"

# Pyspark: Connect using mysql driver
pyspark --driver-class-path /usr/share/java/mysql-connector-java.jar
from pyspark.sql import SQLContext
sqlContext = SQLContext(sc)
jdbcurl = "jdbc:mysql://quickstart.cloudera:3306/retail_db?user=retail_dba&password=cloudera"
df = sqlContext.load(source="jdbc", url=jdbcurl, dbtable="departments")
for rec in df.collect():
  print(rec)


# Execute Python script from Scala context
# Read a file from HDFS location and save it back at different location using Python
  # 1. Create a file and save as .py extension
from pyspark import SparkContext, SparkConf
conf = SparkConf().setAppName("pyspark")
sc = SparkContext(conf=conf)
dataRDD = sc.textFile("/user/cloudera/sqoop_import/departments")
for line in dataRDD.collect():
    print(line)
dataRDD.saveAsTextFile("/user/cloudera/pyspark/departmentsTesting")

  # 2. Run below Spark command
spark-submit --master yarn saveFile.py


# pyspark: Read local file and print
data = sc.textFile("file:///home/cloudera/orders.java")
for line in data.collect():
  print(line)

# Write and read SequenceFile
dataRDD = sc.textFile("/user/cloudera/sqoop_import/departments")
for line in dataRDD.collect():
  print(line)

dataRDD.map(lambda x: tuple(x.split(",", 1))).saveAsSequenceFile("/user/cloudera/pyspark/departmentsAsSequenceFile")
data = sc.sequenceFile("/user/cloudera/pyspark/departmentsAsSequenceFile")
for record in data.collect():
  print(record)

data = sc.sequenceFile("/user/cloudera/pyspark/departmentsAsSequenceFile", "org.apache.hadoop.io.IntWritable", "org.apache.hadoop.io.Text")
for record in data.collect():
    print(record)

# Read data from HiveContext
from pyspark.sql import HiveContext
sqlContext = HiveContext(sc)
for row in sqlContext.sql("select * from departments").collect():
  print(row)

# Read and write JSon files
from pyspark import SQLContext
sqlContext = SQLContext(sc)
departmentsJson = sqlContext.jsonFile("/user/cloudera/pyspark/departments.json")
departmentsJson.registerTempTable("djson")
for row in sqlContext.sql("select * from djson").collect():
  print(row)

sqlContext.sql("select * from djson").toJSON().saveAsTextFile("/user/cloudera/pyspark/departmentsJson")
